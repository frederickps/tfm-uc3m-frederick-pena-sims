---
title: "Analysis"
format: html
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## Analysis

### Library

```{r}
options(scipen = 999)
library(devtools)
library(tidyverse)
library(corrplot)
library(readxl)
library(rDEA)
library(caret)
library(mice)
library(xgboost)
library(mapSpain)
library(kernelshap)
library(shapviz)
library(pdp)
library(factoextra)
```

### Load data (if needed)

```{r}
df_final <- read.csv("Data_2020/TFM_data_2024-06-10.csv", colClasses = c(INE = "character")) |> 
  select(-X)
```

Variable Returns to Scale (RTS = "Variable"): This assumption allows for flexibility in the relationship between inputs and outputs. It recognizes that the impact of municipal spending on reducing unemployment may vary across municipalities due to factors such as local economic conditions, labor market dynamics, and policy effectiveness.

## DEA

### Isotonocity 

The isotonicity property needs to be considered for each input-output pair individually. For Output 1, the data should not violate the isotonicity assumption. For Output 2, since the correlation is slightly negative but close to zero, the impact might be negligible, and the DEA model can still be constructed and interpreted meaningfully.

```{r}
df_iso <- 
  df_final |> 
  select(c(median_inc_con_unit, diff_ue, per_capita_MUN)) |> 
  mutate(diff_ue = (diff_ue * -1)+100)

corr_iso<- cor(df_iso, method = "pearson")
corrplot(corr_iso)
```

### Efficiency score estimation

Braking off into separate data sets and standardizing as is good practice

Difference between two years negated so negative values (reducing unemployment) are observed as better by DEA while greater differences are made negative. Constant added to not infringe on non-negative output.



```{r}
X <- df_final[c("per_capita_MUN")]

Y <- df_final[c("diff_ue", "median_inc_con_unit")]
Y <- 
  Y |> 
  mutate(diff_ue = ((diff_ue * -1)+100))

Z <- df_final[c("dist_prov_cap", "per_capita_CCAA", "per_capita_n_firms")]

# Mean normalization function
mean_normalize <- function(x) {
  return((x - mean(x)) / (max(x) - min(x))+1)
}

# Apply mean normalization to each column
X <- X |>  mutate(across(everything(), mean_normalize))

Z <- Z |>  mutate(across(everything(), mean_normalize))

Y <- Y |> mutate(across(everything(), mean_normalize))
```

#### Validating inverse assumption

```{r}
dea_naive <-
  dea(XREF = X, YREF = Y, X = X, Y = Y, W =NULL, RTS ="variable", model = "output")

df_dea_naive <-
  df_final |> 
  mutate(dea_naive = dea_naive$thetaOpt)
```


```{r}
df_dea_naive |> 
  arrange(desc(dea_naive)) |> 
  select(NOMBRE, CCAA, party_ccaa, party_mun, per_capita_MUN, dist_prov_cap, per_capita_n_firms, diff_ue, median_inc_con_unit, per_capita_CCAA, dea_naive) |> 
  head()
```

#### Environmental DEA estimation

Introducing data frames into Simar and Wilson (2007) model.

```{r}
dea_robust <- 
  dea.env.robust(X, Y, W=NULL, Z, model="output", RTS="variable",
                L1=10, L2=100, alpha=0.05)
```

Calculating the relative efficiency score, as efficiency scores are only provided in absolute terms.

```{r}
sorted_scores <- sort(dea_robust$delta_hat_hat, decreasing = TRUE)
best_score <- max(sorted_scores)
delta_hat_hat <- dea_robust$delta_hat_hat

df_dea <- 
  df_final |> 
  mutate(dea_robust = delta_hat_hat/best_score)
```

## Descriptive Analysis

### Efficiency score

#### Output II - Difference

```{r}
mean(df_dea$diff_ue)
```
On the unemployment rate went down by almost 1%, as backed up by annual data

#### Output II - Median Income per Consumption Unit

```{r}
mean(df_dea$median_inc_con_unit)
```

#### Input - Municipality Expenditure per Capita

```{r}
mean(df_dea$per_capita_MUN)
```


#### Distribution

```{r}
df_dea |> 
  ggplot()+
  geom_histogram(aes(dea_robust), stat = "bin", bins = 160) +
  theme_minimal() +
  labs(title = "Distribution of DEA Efficiency Score",
       y = "Count")
```

#### Most efficient municipality

Finding most efficient municipalities

```{r}
df_dea |> 
  arrange(desc(dea_robust)) |> 
  select(NOMBRE, CCAA, party_ccaa, party_mun, per_capita_MUN, dist_prov_cap, per_capita_n_firms, diff_ue, median_inc_con_unit, per_capita_CCAA, dea_robust) |> 
  head()
```

Finding influence of environmental variables

robust coefficients in the truncated regression of reciprocal of DEA score on environmental variables (after the second loop).

```{r}
dea_robust$beta_hat_hat
```
#### Mean efficiency score

```{r}
mean(df_dea$dea_robust)
```

0.7701055

#### t-test between city sizes

```{r}
greater_20k <- df_dea[df_dea$greater_20k == 1, "dea_robust"]
smaller_20k <- df_dea[df_dea$greater_20k == 0, "dea_robust"]

# Perform the t-test
t_test_result <- t.test(greater_20k, smaller_20k)
t_test_result
```

Smaller municipalities are more efficient, statistically significant.

```{r}
greater_100k <- df_dea[df_dea$pop_total > 100000, "dea_robust"]
smaller_100k <- df_dea[df_dea$pop_total <= 100000, "dea_robust"]

# Perform the t-test
t_test_result <- t.test(greater_100k, smaller_100k)
t_test_result
```

A bit more pronounced if threshold difference between large and small municipality is greater.

#### Map

```{r}
esp_can <- esp_get_country() 
can_prov <- esp_get_can_provinces()
can_box <- esp_get_can_box() 
munic <- esp_get_munic() 

munic <-    
  munic |>  
  rename(INE = LAU_CODE)

df_dea_map <- munic |> 
  left_join(df_dea, by = "INE")  

ggplot(esp_can) +
  geom_sf() +
  geom_sf(data = can_prov) +
  geom_sf(data = can_box) + 
  geom_sf(data = df_dea_map, aes(fill = dea_robust), color = "black", size = .5)+
  scale_fill_gradient(na.value = "grey", low ="#FF0000",high = "#FFFFFF",name = "Efficiency Score") +
  theme_void() +
  labs(title = "Efficiency Score by Municipality (Spain)")
```

Seems to follow the notion that more money does not necessarily produce more efficent labour market outcomes (but often adverse relationship)

North more inefficient than South

### PCA

remove per_capita_revenue

```{r}
df_pca <-
  df_dea |> 
  select_if(is.numeric) |> 
  select(-c(CODAUTO, CPRO, median_inc_con_unit, diff_ue, per_capita_CCAA, dist_prov_cap, n_firms, per_capita_n_firms, per_capita_MUN)) |> 
  drop_na()

set.seed(53456)
pca <- prcomp(df_pca, scale. = T)
```

```{r}
fviz_screeplot(pca, addlabels = TRUE)
```

shows how much information is in each column
only 12% in one column
using the second component 

Filtering less strong rotation values to get better overview

```{r}
# Create a data frame with PCA rotation values
data.frame(variable = rownames(pca$rotation), rotation_value = pca$rotation[, 1]) |>
  filter(rotation_value > 0.15 | rotation_value < -0.15) |> 
  ggplot(aes(x = variable, y = rotation_value)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Barplot of PCA (First Principal Component)",
       x = "Variable", y = "Rotation Value")  +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Filtering less strong rotation values to get better overview
```{r}
data.frame(variable = rownames(pca$rotation), rotation_value = pca$rotation[, 2]) |>
  filter(rotation_value > 0.1 | rotation_value < -0.1) |> 
  ggplot(aes(x = variable, y = rotation_value)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Barplot of PCA (Second Prinicipal Component)",
       x = "Variable", y = "Rotation Value")  +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Higher 

```{r}
data.frame(z1=pca$x[,1],z2=pca$x[,2]) %>% 
  ggplot(aes(z1,z2)) + geom_point(size=0) +
  labs(title="First two principal components (scores)", x="PC1", y="PC2") + #guides(color=guide_legend(title="HDI"))+
  theme_bw() +theme(legend.position="bottom")
```

```{r}
fviz_contrib(pca, choice = "var", axes = 1)
```

DEA does not contribute as much.

```{r}
fviz_contrib(pca, choice = "var", axes = 2)
```

DEA contributes the second most to component number two

### Clustering

```{r}
df_clus <-
  df_dea |> 
  select_if(is.numeric) |> 
  select(-c(CODAUTO, CPRO, diff_ue, median_inc_con_unit, per_capita_CCAA, per_capita_MUN)) |> 
  drop_na()
```

```{r}
fviz_nbclust(scale(df_clus), kmeans, method = 'silhouette', k.max = 40, nstart = 100)
```


```{r}
cluster <- kmeans(df_clus, centers = 2, nstart = 100)
```

Graph

```{r}
# Assuming 'cluster' is your k-means result
centers <- as.data.frame(cluster$centers)
centers$cluster <- factor(1:nrow(centers))  # Add a column for cluster labels

custom_labels <- c(
  general_services = "Share of Spending\nGeneral Services",
  pub_safety_mobility = "Share of Spending\nPublic Safety & Mobility",
  social_services = "Share of Spending\nSocial Services",
  infrastructure = "Share of Spending\nInfrastructure",
  employment_promotion = "Share of Spending\nEmployment Promotion",
  dist_prov_cap = "Distance to\nProvincial Capital",
  ue_pct_2020 = "Unemployment\nRate 2020",
  pop_share_retirees = "Share of Retirees",
  greater_20k = "More than 20,000\n inhabitants",
  share_contracts_serv = "Share of Contracts\nin Service Sector",
  revenue_tax_share = "Municipal\nShare of Revenue:\nTax",
  pop_density = "Population Density",
  dea_robust = "Efficiency Score",
  pop_share_foreigners = "Share of Foreigners",
  share_essential_employment = "Share of\nEssential Employment",
  share_firms_service = "Share of Firms\nin Service Sector"
)


centers |> 
  select(general_services, pub_safety_mobility, social_services, infrastructure, employment_promotion, dist_prov_cap, ue_pct_2020, pop_share_retirees, greater_20k, share_contracts_serv, revenue_tax_share, pop_density, dea_robust, cluster, pop_share_foreigners, share_essential_employment, share_firms_service) |>
  pivot_longer(-cluster, names_to = "variable", values_to = "value") |>
  ggplot(aes(x = variable, y = value, fill = cluster)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  facet_wrap(~ variable, scales = "free", labeller = labeller(variable = custom_labels)) +
  labs(title = "Cluster Centers", y = "Value", fill = "Cluster") +
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        legend.position =  "bottom")
```

### Correlation

Spearman because most of the data is not normally distributed and relationships are not linear

```{r}
df_corr <-
  df_dea |> 
  select(-c(INE, NOMBRE, CPRO, CODAUTO, dea_robust, diff_ue, median_inc_con_unit, per_capita_MUN, per_capita_n_firms, per_capita_CCAA, dist_prov_cap)) |>
  select_if(is.numeric)

corr<- cor(df_dea$dea_robust, df_corr, method = "spearman", use = "complete.obs")
corr<-round(corr,4)

df_corr_results <- 
  data.frame(corr) |> 
  pivot_longer(values_to = "correlation",
               cols = 1:66,
               names_to = "variable")

ggplot(df_corr_results, aes(x = variable, y = correlation)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Correlation with DEA efficiency score", x = "Variable", y = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 0.5)) 
```

# DELETE
### Employment promotion

```{r}
df_dea |> 
  ggplot()+
  geom_histogram(aes(employment_promotion), stat = "bin", bins = 160) +
  theme_minimal()
```

```{r}
df_dea |> 
  ggplot(aes(employment_promotion, dea_robust))+
  geom_point()
```

### Essential Sectors during Covid-19

```{r}
df_dea |> 
  ggplot()+
  geom_histogram(aes(share_essential_business), stat = "bin", bins = 160) +
  theme_minimal()
```

```{r}
df_dea |> 
  ggplot(aes(share_essential_business, dea_robust))+
  geom_point()
```

```{r}
df_dea |> 
  ggplot()+
  geom_histogram(aes(share_essential_employment), stat = "bin", bins = 160) +
  theme_minimal()
```


```{r}
df_dea |> 
  ggplot(aes(share_essential_business, dea_robust))+
  geom_point()
```

### Creating Employment

```{r}
df_dea |> 
  ggplot()+
  geom_histogram(aes(share_ue_agriculture), stat = "bin", bins = 160) +
  theme_minimal()
```

```{r}
df_dea |> 
  ggplot(aes(share_ue_agriculture, dea_robust))+
  geom_point()
```

### Revenue generation

```{r}
df_dea |> 
  ggplot(aes(social_services, dea_robust))+
  geom_point()
```
# DELETE

## Feature Selection and Engineering

### Deleting unnsecessary variables

All variables without additional meaning and variables used in estimating efficiency score+

No revenue data - resembles spending data

```{r}
df_model <- 
  df_dea |> 
  select(-c(CODAUTO, dist_prov_cap, per_capita_CCAA, per_capita_MUN, per_capita_n_firms, median_inc_con_unit, party_mun, party_ccaa, diff_ue))
```

## Data imputation

```{r}
m <- 5
set.seed(475)
imp <- mice(df_model, m = m, method= "rf")
df_model_imp <- complete(imp, action=m)
```

## Preliminary


```{r}
df_model_robust <- df_model_imp |> 
  select(-c(INE, NOMBRE, CCAA, CPRO)) 
```

excluded:
|> 
  filter(!is.na(party_mun)) |> 
  drop_na()
  

## Modelling - Target normal - Random forest

### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train(dea_robust ~ .,
                     data = df_model_robust,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```

#### Data chosen I - CCAA per capita

[1] "party_munOTROS"                    "party_munPSOE"                     "party_munN.ADS."                  
 [4] "party_munUCIN"                     "party_munAxSI"                     "party_munBNG"                     
 [7] "party_munPAR"                      "party_munUPL"                      "party_ccaaERC"                    
[10] "party_ccaaPNV"                     "party_ccaaPSOE"                    "general_services"                 
[13] "environment"                       "social_services"                   "agr_farm_fish"                    
[16] "tax_fin_admin"                     "public_debt"                       "health"                           
[19] "education"                         "infrastructure"                    "employment_promotion"             
[22] "sports"                            "pub_transportation"                "area_sqkm"                        
[25] "ue_pct_2020"                       "pop_share_foreigners"              "pop_share_wk_age"                 
[28] "pop_share_retirees"                "share_essential_business"          "share_essential_employment"       
[31] "rural_prop_tax"                    "special_prop_tax"                  "min_coef_turn_tax"                
[34] "per_capita_ccaa_general_services"  "per_capita_ccaa_security"          "per_capita_ccaa_economic"         
[37] "per_capita_ccaa_culture"           "per_capita_ccaa_social_protection" "pop_density"                      
[40] "per_capita_contracts"              "revenue_tax_share"               

#### Data chosen II
[1] "party_munPP"                  "party_munOTROS"               "party_munPSOE"               
 [4] "party_munN.ADS."              "party_munUCIN"                "party_munAxSI"               
 [7] "party_munBNG"                 "party_munPAR"                 "party_munUPL"                
[10] "party_ccaaERC"                "party_ccaaPNV"                "party_ccaaPRC"               
[13] "party_ccaaPSOE"               "general_services"             "environment"                 
[16] "social_services"              "agr_farm_fish"                "tax_fin_admin"               
[19] "public_debt"                  "health"                       "education"                   
[22] "infrastructure"               "employment_promotion"         "sports"                      
[25] "pub_transportation"           "area_sqkm"                    "ue_pct_2020"                 
[28] "pop_share_foreigners"         "pop_share_wk_age"             "pop_share_retirees"          
[31] "share_essential_business"     "share_essential_employment"   "rural_prop_tax"              
[34] "special_prop_tax"             "min_coef_turn_tax"            "share_ccaa_general_services" 
[37] "share_ccaa_economic"          "share_ccaa_culture"           "share_ccaa_social_protection"
[40] "pop_density"                  "per_capita_contracts"         "revenue_tax_share" 

#### Data chosen III
 [1] "party_munPP"                  "party_munOTROS"               "party_munPSOE"                "party_munN.ADS."              "party_munUCIN"               
 [6] "party_munBNG"                 "party_munPAR"                 "party_munUPL"                 "party_ccaaERC"                "party_ccaaPNV"               
[11] "party_ccaaPSOE"               "general_services"             "environment"                  "social_services"              "agr_farm_fish"               
[16] "tax_fin_admin"                "public_debt"                  "health"                       "education"                    "infrastructure"              
[21] "employment_promotion"         "sports"                       "pub_transportation"           "pensions"                     "r_and_d"                     
[26] "area_sqkm"                    "ue_pct_2020"                  "share_ue_construction"        "share_ue_industry"            "share_ue_agriculture"        
[31] "share_ue_wo_job"              "pop_share_foreigners"         "pop_share_wk_age"             "pop_share_retirees"           "share_women"                 
[36] "share_essential_employment"   "share_firms_service"          "share_contracts_agri"         "share_contracts_con"          "rural_prop_tax"              
[41] "special_prop_tax"             "min_coef_turn_tax"            "revenue_tax_share"            "share_ccaa_general_services"  "share_ccaa_economic"         
[46] "share_ccaa_culture"           "share_ccaa_social_protection" "pop_density"                  "per_capita_contracts"    

#### Data chosen IV - Ideology score

 [1] "general_services"            "environment"                 "social_services"             "agr_farm_fish"               "tax_fin_admin"              
 [6] "public_debt"                 "health"                      "education"                   "infrastructure"              "employment_promotion"       
[11] "trans_public_agencies"       "other_econ"                  "pub_transportation"          "r_and_d"                     "area_sqkm"                  
[16] "ue_pct_2020"                 "share_ue_construction"       "share_ue_industry"           "share_ue_agriculture"        "pop_share_foreigners"       
[21] "pop_share_wk_age"            "pop_share_retirees"          "share_women"                 "share_essential_employment"  "share_firms_service"        
[26] "share_contracts_agri"        "share_contracts_con"         "rural_prop_tax"              "special_prop_tax"            "revenue_tax_share"          
[31] "revenue_grants_share"        "share_ccaa_general_services" "share_ccaa_security"         "share_ccaa_economic"         "share_ccaa_housing_services"
[36] "share_ccaa_culture"          "pop_density"                 "per_capita_contracts"        "ideology_score"  
Data chosen V - n_firms exogenous variable

[1] "general_services"            "environment"                 "social_services"            
 [4] "agr_farm_fish"               "tax_fin_admin"               "public_debt"                
 [7] "health"                      "education"                   "infrastructure"             
[10] "employment_promotion"        "other_econ"                  "pub_transportation"         
[13] "r_and_d"                     "area_sqkm"                   "ue_pct_2020"                
[16] "share_ue_construction"       "share_ue_industry"           "share_ue_agriculture"       
[19] "pop_share_foreigners"        "pop_share_wk_age"            "pop_share_retirees"         
[22] "greater_20k1"                "share_women"                 "share_essential_employment" 
[25] "share_firms_service"         "share_contracts_agri"        "share_contracts_con"        
[28] "urban_prop_tax"              "rural_prop_tax"              "special_prop_tax"           
[31] "min_coef_turn_tax"           "revenue_tax_share"           "party_ccaa_id"              
[34] "share_ccaa_general_services" "share_ccaa_security"         "share_ccaa_economic"        
[37] "share_ccaa_housing_services" "share_ccaa_culture"          "pop_density"                
[40] "per_capita_contracts"  

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "party")&!startsWith(non_zero_vars, "greater")]


df_model_final <- 
  df_model_imp |>
  dplyr::select(all_of(non_zero_vars), dea_robust, greater_20k, INE, NOMBRE) |> 
  drop_na()
# excluded , party_mun,party_ccaa for the moment

# all_of(non_zero_vars), dea_robust, greater_20k, party_mun, party_ccaa, INE, NOMBRE
```

Hypothesis: covid employment data makes models overfit --\> too accurate AND decreases R-squared. How?


#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_model_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data  <- df_model_final[training.samples, ]
test.data <- df_model_final[-training.samples, ] 

# deleting INE and NOMBRE from train set to keep in test only
train.data$INE <- NULL
train.data$NOMBRE <- NULL

```

#### Training RF
```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune <- train(dea_robust ~., 
                 data = train.data,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(15,16,17,18)),
                 importance = TRUE)

rf_tune$finalModel$importance

rf_tune$results

varImp(rf_tune)

plot(varImp(rf_tune))
```

##### Results

```{r}
test_results <- data.frame(dea_robust = test.data$dea_robust)

#normal
test_results$rf <- predict(rf_tune, test.data)
rf_normal <- postResample(pred = test_results$rf,  obs = test_results$dea_robust)
```

See data chosen I
0.6485436 
MAE
0.03261175

see data chosen I
Normal
 RMSE   Rsquared        MAE 
0.04348254 0.63551313 0.03342641 

Data chosen II
0.6504064
MAE
0.03264785

see data chosen II
Normal
      RMSE   Rsquared        MAE 
0.04340711 0.63832142 0.03360933 

Data chosen III
0.6552379
MAE
0.03280619
    RMSE   Rsquared        MAE 
0.04294293 0.64908921 0.03300969

Data chosen IV
0.6574947
MAE
0.03240815
   RMSE   Rsquared        MAE 
0.04262793 0.65125692 0.03257174 


--\> constant problem: overfitting (reduced with additional data)

-   **MAE Interpretation:** MAE (Mean Absolute Error) represents the average difference between the predicted values and the actual values. In your case, the average difference is only 0.034, which is a very small portion of the target variable's range (0.5 to 1).

-   **Relative Error:** Considering the limited range of the target variable (from 0.5 to 1), an absolute error of 0.034 is even more significant. It translates to a maximum relative error of around 6.8% (0.034 / (1-0.5) ).

**General Rule of Thumb:**

-   A lower MAE indicates better model performance. In general, an MAE less than 10% of the variable's range is considered good. Here, your MAE is well below that threshold.

Prediction

## Modelling - Target log - Random forest

### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train(log(dea_robust) ~ .,
                     data = df_model_robust,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```
#### Data chosen I
Did not do first two correctly

 [1] "party_munPP"                  "party_munOTROS"               "party_munPSOE"                "party_munN.ADS."              "party_munUCIN"               
 [6] "party_munBNG"                 "party_munPAR"                 "party_munUPL"                 "party_munAHORA DECIDE"        "party_ccaaERC"               
[11] "party_ccaaPNV"                "party_ccaaPSOE"               "general_services"             "environment"                  "social_services"             
[16] "culture"                      "agr_farm_fish"                "tax_fin_admin"                "public_debt"                  "health"                      
[21] "education"                    "infrastructure"               "employment_promotion"         "sports"                       "trans_public_agencies"       
[26] "pub_transportation"           "pensions"                     "r_and_d"                      "area_sqkm"                    "ue_pct_2020"                 
[31] "share_ue_construction"        "share_ue_industry"            "share_ue_agriculture"         "share_ue_wo_job"              "pop_share_foreigners"        
[36] "pop_share_wk_age"             "pop_share_retirees"           "share_women"                  "share_essential_employment"   "share_firms_service"         
[41] "share_contracts_agri"         "share_contracts_con"          "rural_prop_tax"               "special_prop_tax"             "min_coef_turn_tax"           
[46] "revenue_tax_share"            "share_ccaa_general_services"  "share_ccaa_economic"          "share_ccaa_culture"           "share_ccaa_social_protection"
[51] "pop_density"                  "per_capita_contracts"

#### Data chosen II - ideology score
 [1] "general_services"             "environment"                  "social_services"              "agr_farm_fish"                "tax_fin_admin"               
 [6] "public_debt"                  "health"                       "education"                    "infrastructure"               "employment_promotion"        
[11] "sports"                       "trans_public_agencies"        "other_econ"                   "pub_transportation"           "pensions"                    
[16] "r_and_d"                      "area_sqkm"                    "ue_pct_2020"                  "share_ue_service"             "share_ue_construction"       
[21] "share_ue_industry"            "share_ue_agriculture"         "pop_share_foreigners"         "pop_share_wk_age"             "pop_share_retirees"          
[26] "share_women"                  "share_essential_employment"   "share_firms_service"          "share_contracts_agri"         "share_contracts_con"         
[31] "urban_prop_tax"               "rural_prop_tax"               "special_prop_tax"             "min_coef_turn_tax"            "revenue_tax_share"           
[36] "revenue_grants_share"         "share_ccaa_general_services"  "share_ccaa_security"          "share_ccaa_economic"          "share_ccaa_housing_services" 
[41] "share_ccaa_culture"           "share_ccaa_social_protection" "pop_density"                  "per_capita_contracts"         "ideology_score" 

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "party")&!startsWith(non_zero_vars, "greater")]


df_model_final <- 
  df_model_imp |>
  dplyr::select(all_of(non_zero_vars), dea_robust, INE, NOMBRE)|>
  drop_na()
# excluded , party_mun,party_ccaa for the moment
```

filter(!is.na(party_mun)) |> 


#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_model_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data  <- df_model_final[training.samples, ]
test.data <- df_model_final[-training.samples, ] 

# deleting INE and NOMBRE from train set to keep in test only
train.data$INE <- NULL
train.data$NOMBRE <- NULL

```

#### Training RF
```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune_log <- train(log(dea_robust) ~., 
                 data = train.data,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(9,12,15,18,21)),
                 importance = TRUE)

rf_tune_log$finalModel$importance

rf_tune_log$results

varImp(rf_tune_log)

plot(varImp(rf_tune_log))
```

##### Results

```{r}
# log
test_results$rf_log <- predict(rf_tune_log, test.data)
test_results$rf_log <- exp(test_results$rf_log)
rf_log <- postResample(pred = test_results$rf_log,  obs = test_results$dea_robust)
```
see data chosen I
0.6648428
MAE
0.04230489
    RMSE  Rsquared       MAE 
1.0353495 0.6472887 1.0344253

data chosen II
0.6637780
MAE
0.04170904
 RMSE   Rsquared        MAE 
0.04204950 0.66050201 0.03215307 

## Modelling - Target log10 - Random forest
### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train(log10(dea_robust) ~ .,
                     data = df_model_robust,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```
#### Data chosen I
Did not do first two correctly

[1] "party_munPSOE"                "party_munPAR"                 "party_ccaaERC"                "party_ccaaPNV"                "party_ccaaPSOE"              
 [6] "environment"                  "social_services"              "agr_farm_fish"                "tax_fin_admin"                "public_debt"                 
[11] "health"                       "infrastructure"               "employment_promotion"         "pub_transportation"           "area_sqkm"                   
[16] "ue_pct_2020"                  "share_ue_construction"        "share_ue_industry"            "share_ue_agriculture"         "pop_share_foreigners"        
[21] "pop_share_wk_age"             "pop_share_retirees"           "share_women"                  "share_essential_employment"   "share_firms_service"         
[26] "share_contracts_agri"         "share_contracts_con"          "rural_prop_tax"               "special_prop_tax"             "revenue_tax_share"           
[31] "revenue_grants_share"         "share_ccaa_general_services"  "share_ccaa_security"          "share_ccaa_economic"          "share_ccaa_culture"          
[36] "share_ccaa_social_protection" "pop_density"                  "per_capita_contracts" 

data chosen II
[1] "environment"                 "social_services"             "agr_farm_fish"               "tax_fin_admin"               "public_debt"                
 [6] "health"                      "infrastructure"              "employment_promotion"        "pub_transportation"          "area_sqkm"                  
[11] "ue_pct_2020"                 "share_ue_construction"       "share_ue_industry"           "share_ue_agriculture"        "pop_share_foreigners"       
[16] "pop_share_wk_age"            "pop_share_retirees"          "share_women"                 "share_essential_employment"  "share_firms_service"        
[21] "share_contracts_agri"        "share_contracts_con"         "rural_prop_tax"              "special_prop_tax"            "revenue_tax_share"          
[26] "revenue_grants_share"        "share_ccaa_general_services" "share_ccaa_security"         "share_ccaa_economic"         "share_ccaa_housing_services"
[31] "share_ccaa_culture"          "pop_density"                 "per_capita_contracts"

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "party")&!startsWith(non_zero_vars, "greater")]


df_model_final <- 
  df_model_imp |>
  dplyr::select(all_of(non_zero_vars), dea_robust, INE, NOMBRE)|> 
  drop_na()
# excluded party ccaa for the moment
```

Hypothesis: covid employment data makes models overfit --\> too accurate AND decreases R-squared. How?

|> 
  filter(!is.na(party_mun)) 

#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_model_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data  <- df_model_final[training.samples, ]
test.data <- df_model_final[-training.samples, ] 

# deleting INE and NOMBRE from train set to keep in test only
train.data$INE <- NULL
train.data$NOMBRE <- NULL

```

#### Training RF
```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune_log10 <- train(log10(dea_robust) ~., 
                 data = train.data,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(9,12,15,18,21)),
                 importance = TRUE)

rf_tune_log10$finalModel$importance

rf_tune_log10$results

varImp(rf_tune_log10)

plot(varImp(rf_tune_log10))
```

##### Results

```{r}
# log10
test_results$rf_log10 <- predict(rf_tune_log10, test.data)
test_results$rf_log10 <- 10^(test_results$rf_log10)
rf_log10 <- postResample(pred = test_results$rf_log10,  obs = test_results$dea_robust)
```

0.6638687
0.01834957
  RMSE  Rsquared       MAE 
0.8862600 0.6474117 0.8848043 

data chosen II - ideology score
0.6631807
MAE
0.01817255
RMSE   Rsquared        MAE 
0.04228648 0.65581593 0.03220443 

## Target cube root - Random forest

### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train((dea_robust)^(1/3) ~ .,
                     data = df_model_robust,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```
#### Data chosen I
Did not do first two correctly

  [1] "party_munPSOE"                "party_ccaaERC"                "party_ccaaPNV"                "party_ccaaPSOE"               "social_services"             
 [6] "tax_fin_admin"                "public_debt"                  "health"                       "infrastructure"               "employment_promotion"        
[11] "area_sqkm"                    "ue_pct_2020"                  "share_ue_construction"        "share_ue_industry"            "share_ue_agriculture"        
[16] "pop_share_foreigners"         "pop_share_wk_age"             "pop_share_retirees"           "share_essential_employment"   "share_contracts_agri"        
[21] "share_contracts_con"          "rural_prop_tax"               "revenue_tax_share"            "revenue_grants_share"         "share_ccaa_general_services" 
[26] "share_ccaa_security"          "share_ccaa_economic"          "share_ccaa_culture"           "share_ccaa_social_protection" "pop_density"                 
[31] "per_capita_contracts" 

data chosen II - ideology score

[1] "social_services"             "tax_fin_admin"               "public_debt"                 "health"                      "infrastructure"             
 [6] "employment_promotion"        "area_sqkm"                   "ue_pct_2020"                 "share_ue_construction"       "share_ue_industry"          
[11] "share_ue_agriculture"        "pop_share_foreigners"        "pop_share_wk_age"            "pop_share_retirees"          "share_women"                
[16] "share_essential_employment"  "share_firms_service"         "share_contracts_agri"        "share_contracts_con"         "rural_prop_tax"             
[21] "special_prop_tax"            "revenue_tax_share"           "revenue_grants_share"        "share_ccaa_general_services" "share_ccaa_security"        
[26] "share_ccaa_economic"         "share_ccaa_culture"          "pop_density"                 "per_capita_contracts" 

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "party")&!startsWith(non_zero_vars, "greater")]


df_model_final <- 
  df_model_imp |>
  dplyr::select(all_of(non_zero_vars), dea_robust, INE, NOMBRE)|> 
  drop_na()
# excluded party ccaa for the moment
```

Hypothesis: covid employment data makes models overfit --\> too accurate AND decreases R-squared. How?


#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_model_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data  <- df_model_final[training.samples, ]
test.data <- df_model_final[-training.samples, ] 

# deleting INE and NOMBRE from train set to keep in test only
train.data$INE <- NULL
train.data$NOMBRE <- NULL

```

#### Training RF
```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune_cube <- train((dea_robust)^(1/3) ~., 
                 data = train.data,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(9,12,15,18,21)),
                 importance = TRUE)

rf_tune_cube$finalModel$importance

rf_tune_cube$results

varImp(rf_tune_cube)

plot(varImp(rf_tune_cube))
```

##### Results
```{r}
# cube
test_results$rf_cube <- predict(rf_tune_cube, test.data)
test_results$rf_cube <- (test_results$rf_cube)^3
rf_cube <- postResample(pred = test_results$rf_cube,  obs = test_results$dea_robust)
```


0.6542624
MAE
0.01300218
     RMSE  Rsquared       MAE 
0.1562569 0.6477807 0.1460189 

data chosen II
0.6619849
MAE
0.01284411
RMSE   Rsquared        MAE 
0.04242569 0.65422809 0.03235297 


#### Compare Results

```{r}
cube_df <- as.data.frame(t(rf_cube))
log_df <- as.data.frame(t(rf_log))
normal_df <- as.data.frame(t(rf_normal))
log10_df <- as.data.frame(t(rf_log10))

cube_df$model <- "Cube Root"
log_df$model <- "Log"
normal_df$model <- "Normal"
log10_df$model <- "Log10"

rf_metrics_table <- rbind(cube_df, log_df, normal_df, log10_df)

rf_metrics_table
```


#### Visualizing best

```{r}
qplot(test_results$rf, test_results$dea_robust) + 
  labs(title="RF Regression Observed VS Predicted", x="Predicted", y="Observed") +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

Problem --> Absolute error

## MAE Investigation

```{r}
# normal
mae_per_observation_rf <- abs(test_results$rf - test_results$dea_robust)
test.data$mae_rf <- mae_per_observation_rf 
```

```{r}
df_mae <- 
  test.data |> 
  select(INE, NOMBRE, mae_rf) |> 
  left_join(df_dea, by = c("INE", "NOMBRE")) |> 
  drop_na()
```

### Correlation Analysis

```{r}
df_corr_mae <-
  df_mae |> 
  select_if(is.numeric) |> 
  select(-mae_rf)

corr_mae<- cor(df_mae$mae_rf, df_corr_mae, method = "spearman")
corr_mae<-round(corr_mae,4)

df_corr_mae_results <- 
  data.frame(corr_mae) |> 
  pivot_longer(values_to = "correlation",
               cols = 1:69,
               names_to = "variable")

ggplot(df_corr_mae_results, aes(x = variable, y = correlation)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Correlation with absolute error", x = "Variable", y = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 0.5)) 
```

#### dea_robust variable

```{r}
df_mae |> 
  ggplot()+
  geom_point(aes(x = dea_robust, y = mae_rf)) +
  theme_minimal()
```

#### Mapping MAE

```{r}
esp_can <- esp_get_country() 
can_prov <- esp_get_can_provinces()
can_box <- esp_get_can_box() 
munic <- esp_get_munic() 

munic <-    
  munic |>  
  rename(INE = LAU_CODE)

df_mae_map <- munic |> 
  left_join(df_mae, by = "INE")  

ggplot(esp_can) +
  geom_sf() +
  geom_sf(data = can_prov) +
  geom_sf(data = can_box) + 
  geom_sf(data = df_mae_map, aes(fill = mae_rf), color = "black", size = .5)+
  scale_fill_viridis_c(na.value = NA) +
  theme_minimal()
```

No spatial relationship

```{r}
df_mae |> 
  ggplot()+
  geom_point(aes(x = area_sqkm, y = mae_rf)) +
  theme_minimal()+
    coord_cartesian(xlim = c(0, 300), ylim = c(0, 0.18))
```

### Linear regression

```{r}
df_mae_lm <-
  na.omit(df_mae) |> 
  select(-c(INE, NOMBRE, CCAA, party_ccaa, party_mun, median_inc_con_unit, target, dea_robust))
model <- lm(mae_rf ~ ., data = df_mae_lm)

# Print summary of the model
summary(model)
```
### Absoulte error by Regions (Province and Autonomous Community)

#### Province

```{r}
CPRO_mae <- df_mae |> 
  group_by(CPRO) |> 
  summarise(mae_rf = mean(mae_rf, na.rm = TRUE))

# Create the graph
ggplot(CPRO_mae, aes(x = CPRO, y = mae_rf)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Mean Absolute Error by Province",
       x = "Regional Area (CPRO)",
       y = "Mean Absolute Error") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Map

```{r}
esp_can <- esp_get_country() 
can_prov <- esp_get_can_provinces()
can_box <- esp_get_can_box() 
provs <- esp_get_prov_siane(epsg = 3857)

df_mae_prov <- provs |> 
  left_join(CPRO_mae, by = c("cpro" = "CPRO"))  

ggplot(esp_can) +
  geom_sf() +
  geom_sf(data = can_prov) +
  geom_sf(data = can_box) + 
  geom_sf(data = df_mae_prov, aes(fill = mae_rf), color = "black", size = .5)+
  scale_fill_viridis_c(na.value = NA) +
  theme_minimal()
```

#### Autnomous communities

```{r}
CCAA_mae <- df_mae |> 
  group_by(CCAA, CODAUTO) |> 
  summarise(mae_rf = mean(mae_rf, na.rm = TRUE))

# Create the graph
ggplot(CCAA_mae, aes(x = CCAA, y = mae_rf)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Mean Absolute Error by CCAA",
       x = "Autonomous Community",
       y = "Mean Absolute Error") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Mapping
```{r}
esp_can <- esp_get_country() 
can_prov <- esp_get_can_provinces()
can_box <- esp_get_can_box() 
CCAA_sf <- esp_get_ccaa()

df_mae_ccaa <- CCAA_sf |> 
  left_join(CCAA_mae, by = c("codauto" = "CODAUTO"))  

ggplot(esp_can) +
  geom_sf() +
  geom_sf(data = can_prov) +
  geom_sf(data = can_box) + 
  geom_sf(data = df_mae_ccaa, aes(fill = mae_rf), color = "black", size = .5)+
  scale_fill_viridis_c(na.value = NA) +
  theme_minimal()
```

### Scatter plots
```{r}
ggplot(df_mae) +
    geom_point(aes(x = pop_density, y = mae_rf)) +
    theme_minimal() +
    coord_cartesian(xlim = c(0, 300), ylim = c(0, 0.18))
```

```{r}
ggplot(df_mae) +
    geom_point(aes(x = ue_pct_2020, y = mae_rf)) +
    theme_minimal() +
    coord_cartesian(xlim = c(0, 60), ylim = c(0, 0.18))
```

```{r}
df_mae |> 
  arrange(desc(MAE))
```

## Building the Model

## Feature Engineering

Selecting imputed data frame while keeping CCAA and CPRO

```{r}
df_ml <- 
  df_model_imp |> 
  select(-c(INE, NOMBRE, CCAA)) |> 
  mutate(CPRO = as.factor(CPRO))
```

Adding features

bivariat:

pop_density --\> threshold: 50

area_sqkm --\> trheshold: 50

pop_total --\> threshold: 500

keep CPRO

Interactions:

All of the above with all variables except pca and dea

Function
```{r}
generate_interactions <- function(df) {
  # Extracting numeric columns excluding dea_robust
  numeric_cols <- df %>%
    select(where(is.numeric), -dea_robust)
  
  # Extracting pop_total column
  pop_total_col <- numeric_cols %>%
    select(pop_total, pop_density, area_sqkm)
  
  # Other numeric columns excluding pop_total
  other_numeric_cols <- numeric_cols %>%
    select(-c(pop_total, pop_density, area_sqkm))
  
  # Generating interactions
  interaction_cols <- list()
  
  for(col_name in names(other_numeric_cols)) {
    interaction_name <- paste("pop_total", col_name, sep = "_x_")
    interaction_name <- make.names(interaction_name, unique = TRUE)  # Ensure unique column names
    interaction_cols[[interaction_name]] <- pop_total_col[[1]] * other_numeric_cols[[col_name]]
  }
  
  # Combining original and interaction columns
  df_interactions <- cbind(df, as.data.frame(interaction_cols))
  
  return(df_interactions)
}
```

```{r}
# to apply feature engineering
vars_to_engineer <- 
  df_ml |> 
  select(where(is.numeric)) |> 
  names()

# feature engineering
df_ml <- 
  df_ml |> 
  mutate(pop_density_low = as.factor(if_else(pop_density < 50, 1,0)), # adding varaibles
         pop_total_low = as.factor(if_else(pop_total < 500, 1,0)),
         area_sqkm_low = as.factor(if_else(area_sqkm < 50, 1,0))) |>
  generate_interactions() |> 
  mutate(across(all_of(vars_to_engineer), ~log(. + 1), .names = "{.col}_log"), # feature engineering mathematical alteration of variables
         across(all_of(vars_to_engineer), ~.^2, .names = "{.col}_2"),
         across(all_of(vars_to_engineer), ~.^3, .names = "{.col}_3")) |> 
  mutate(pop_total_x_area_sqkm = area_sqkm*pop_total,
         pop_density_X_area_sqkm = pop_density * area_sqkm,
         pop_total_x_pop_density = pop_total*pop_density) |> 
  drop_na()

df_ml$dea_robust_2 <- NULL
df_ml$dea_robust_log <- NULL
df_ml$dea_robust_3 <- NULL
```

## Feature selection

### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train(dea_robust ~ .,
                     data = df_ml,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```

#### Data chosen
[1] "CPRO5"                             "CPRO6"                             "CPRO9"                             "CPRO10"                           
 [5] "CPRO11"                            "CPRO12"                            "CPRO13"                            "CPRO14"                           
 [9] "CPRO15"                            "CPRO16"                            "CPRO18"                            "CPRO19"                           
[13] "CPRO21"                            "CPRO22"                            "CPRO23"                            "CPRO24"                           
[17] "CPRO29"                            "CPRO31"                            "CPRO33"                            "CPRO34"                           
[21] "CPRO35"                            "CPRO37"                            "CPRO38"                            "CPRO39"                           
[25] "CPRO40"                            "CPRO41"                            "CPRO42"                            "CPRO43"                           
[29] "CPRO45"                            "CPRO47"                            "CPRO48"                            "social_services"                  
[33] "tax_fin_admin"                     "health"                            "employment_promotion"              "pensions"                         
[37] "share_ue_industry"                 "pop_share_foreigners"              "share_contracts_agri"              "share_contracts_con"              
[41] "share_ccaa_general_services"       "pop_density_low1"                  "area_sqkm_low1"                    "pop_total_x_gov_bodies"           
[45] "pop_total_x_trans_public_agencies" "pop_total_x_pop_share_foreigners"  "pop_total_x_share_ccaa_economic"   "CPRO_log"                         
[49] "environment_log"                   "social_services_log"               "public_debt_log"                   "gov_bodies_log"                   
[53] "pub_transportation_log"            "ue_pct_2020_log"                   "share_ue_construction_log"         "share_ue_industry_log"            
[57] "share_ue_agriculture_log"          "share_ue_wo_job_log"               "pop_share_wk_age_log"              "pop_share_youth_log"              
[61] "share_firms_service_log"           "share_contracts_con_log"           "urban_prop_tax_log"                "revenue_tax_share_log"            
[65] "revenue_other_share_log"           "share_ccaa_economic_log"           "pop_density_log"                   "per_capita_contracts_log"         
[69] "general_services_2"                "health_2"                          "infrastructure_2"                  "rural_prop_tax_2"                 
[73] "share_ccaa_general_services_2"     "per_capita_contracts_2"            "general_services_3"                "housing_urban_plan_3"             
[77] "area_sqkm_3"                       "share_ue_industry_3"               "pop_share_foreigners_3"            "pop_share_youth_3"                
[81] "share_firms_service_3"             "share_contracts_serv_3"            "revenue_grants_share_3"            "share_ccaa_security_3"            
[85] "share_ccaa_social_protection_3"   


```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "CPRO")&!startsWith(non_zero_vars, "greater")&!endsWith(non_zero_vars, "low1")]

df_ml_final <- 
  df_ml |> 
  dplyr::select(all_of(non_zero_vars), dea_robust, CPRO, area_sqkm_low, pop_density_low, greater_20k, pop_total_x_pop_density, pop_total_x_area_sqkm, pop_density_X_area_sqkm)
```

Keeping greater_20k beause of previous insights about 


#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_ml_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data  <- df_ml_final[training.samples, ]
test.data <- df_ml_final[-training.samples, ] 
```

### Training Random Forest

```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune_final <- train(dea_robust ~., 
                 data = train.data,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 1000,
                 tuneGrid = data.frame(mtry= c(45:58)),
                 importance = TRUE)

```

```{r}
rf_tune_final$finalModel$importance

summary(rf_tune_final$finalModel)

plot(rf_tune_final)
```

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 56.

#### Results
67.1

RMSE   Rsquared        MAE 
0.03868612 0.69773528 0.02959235


```{r}
model_results <- data.frame(dea_robust = test.data$dea_robust)

model_results$rf <- predict(rf_tune_final, test.data)
postResample(pred = model_results$rf,  obs = model_results$dea_robust)
```
### XG Boost

```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(123)
xgb_tune <- train(dea_robust ~., 
                  data = train.data,
                  method = "xgbTree",
                  preProc=c('scale','center'),
                  objective = "reg:squarederror",
                  trControl = ctrl,
                  tuneGrid = expand.grid(nrounds = c(1000, 1500), 
                                         max_depth = c(6,8,10), 
                                         eta = c(0.005, 0.01, 0.1),
                                         gamma = c(0.025, 0.001, 0.01),
                                         colsample_bytree = c(0.4, 0.6, 0.8),
                                         min_child_weight = c(1, 3, 5),
                                         subsample = c( 0.7, 0.8)))
```

```{r}
model_results$xg_boost <- predict(xgb_tune, test.data)
postResample(pred = model_results$xg_boost,  obs = model_results$dea_robust)
```


```{r}
xgb_tune$results

varImp(xgb_tune)

plot(varImp(xgb_tune))

plot(xgb_tune)
```


### Catboost

```{r}
catboost_model <- catboost(
  formula = target ~ .,
  data = train_data,
  verbose = FALSE, # Set to TRUE for verbose output
  iterations = 100, # Number of boosting iterations
  learning_rate = 0.1, # Learning rate
  depth = 6, # Depth of trees
  loss_function = "RMSE" # Loss function
)
```


## Interpretation

```{r}
varImp(rf_tune_final)

plot(varImp(rf_tune_final))
```


### PDP

```{r}
partial(rf_tune_final, pred.var = "employment_promotion", plot = TRUE, rug = T, train = train.data)
```

### SHAP

Importance Bee Plot

```{r}
X <- train.data[sample(nrow(train.data), 1000), -which(names(train.data) == "dea_robust")]
bg_X <- train.data[sample(nrow(train.data), 100), -which(names(train.data) == "dea_robust")]

s <- kernelshap(rf_tune, X = X, bg_X = bg_X) 
sv <- shapviz(s)
sv_importance(sv, kind = "bee", max_display = Inf)
```

Dependence

```{r}
v_select_1 <- c("per_capita_contracts", "pop_density", "pop_share_wk_age", "area_sqkm", "rural_prop_tax", "social_services", "employment_promotion", "party_mun")
sv_dependence(sv, v = v_select_1)
```

### Under and overachievers

```{r}

yhat = test_results$rf
hist(yhat, col="lightblue")

y = test_results$dea_robust
error = y-yhat
hist(error, col="lightblue")

noise <- error[1:100]

lwr <- yhat[101:length(yhat)] + quantile(noise,0.05, na.rm=T)
upr <- yhat[101:length(yhat)] + quantile(noise,0.95, na.rm=T)

predictions = data.frame(real=y[101:length(y)], fit=yhat[101:length(yhat)], lwr=lwr, upr=upr)

predictions = predictions %>% mutate(out=factor(if_else(real<lwr | real>upr,1,0)))

mean(predictions$out==1)

ggplot(predictions, aes(x=fit, y=real))+
  geom_point(aes(color=out)) + theme(legend.position="none") +
  geom_ribbon(data=predictions,aes(ymin=lwr,ymax=upr),alpha=0.3) +
  labs(title = "Prediction intervals", x = "Prediction",y="Estimated Efficiency")
```

## Appendix

### Historical Unemployment Rate Development

```{r}
# Read the file line by line
lines <- readLines("Data_2020/world_ue_data.csv", encoding = "UTF-8")

# Extract column names from the third line
column_names <- unlist(strsplit(lines[5], ",", fixed = TRUE))
column_names <- gsub('"', "", column_names)  # Remove quotes
column_names <- gsub("\\\\", "", column_names)

# to shift columns one column to the right
column_names <- c("country_name", column_names)


# Split each line into columns based on commas
data <- lapply(lines[-(1:5)], function(line) {
  unlist(strsplit(line, ",", fixed = TRUE))
})

# Convert the list to a data frame and assign column names
data <- as.data.frame(do.call(rbind, data))
names(data) <- column_names


data |> 
  select("country_name","1990":"2021") |> 
  mutate(across("1990":"2021", ~str_extract(., "\\d+\\.?\\d*"))) |> 
  filter(country_name == '"Spain') |> 
  pivot_longer(cols = "1990":"2021",
               values_to = "ue_rate",
               names_to = "year") |> 
  mutate(ue_rate = as.numeric(ue_rate)/100,
         year = as.Date(paste0(year, "-01-01"))) |> 
  ggplot()+
  geom_line(aes(x = year, y = ue_rate), colour = "steelblue") +
  labs(title = "Unemployment Rate 1990 - 2021 (Spain)",
       y = "",
       x = "") +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_x_date(date_labels = "%Y", breaks = "1 year", expand = c(0, 0)) +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

