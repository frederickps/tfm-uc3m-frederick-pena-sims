---
title: "Analysis"
format: html
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## Analysis

### Install unusual packages

```{r}
install.packages('remotes')
remotes::install_url('https://github.com/catboost/catboost/releases/download/v1.2.5/catboost-R-windows-x86_64-1.2.5.tgz', INSTALL_opts = c("--no-multiarch", "--no-test-load"))
```

```{r}
if (!requireNamespace("catboost", quietly = TRUE)) {
  install.packages("catboost")
}
if (!requireNamespace("caret", quietly = TRUE)) {
  install.packages("caret")
}
if (!requireNamespace("xgboost", quietly = TRUE)) {
  install.packages("xgboost")
}
```


### Library

```{r}
options(scipen = 999)
library(catboost)
library(devtools)
library(tidyverse)
library(corrplot)
library(readxl)
library(rDEA)
library(caret)
library(mice)
library(xgboost)
library(mapSpain)
library(kernelshap)
library(shapviz)
library(pdp)
library(factoextra)
library(patchwork)
```

### Load data (if needed)

```{r}
df_final <- read.csv("Data_2020/TFM_data_2024-06-10.csv", colClasses = c(INE = "character")) |> 
  select(-X)
```

Variable Returns to Scale (RTS = "Variable"): This assumption allows for flexibility in the relationship between inputs and outputs. It recognizes that the impact of municipal spending on reducing unemployment may vary across municipalities due to factors such as local economic conditions, labor market dynamics, and policy effectiveness.

## DEA

### Isotonocity 

The isotonicity property needs to be considered for each input-output pair individually. For Output 1, the data should not violate the isotonicity assumption. For Output 2, since the correlation is slightly negative but close to zero, the impact might be negligible, and the DEA model can still be constructed and interpreted meaningfully.

```{r}
df_iso <- 
  df_final |> 
  select(c(median_inc_con_unit, diff_ue, per_capita_MUN)) |> 
  mutate(diff_ue = (diff_ue * -1)+100)

corr_iso<- cor(df_iso, method = "pearson")
corrplot(corr_iso)
```

### Efficiency score estimation

Braking off into separate data sets and standardizing as is good practice

Difference between two years negated so negative values (reducing unemployment) are observed as better by DEA while greater differences are made negative. Constant added to not infringe on non-negative output.



```{r}
X <- df_final[c("per_capita_MUN")]

Y <- df_final[c("diff_ue", "median_inc_con_unit")]
Y <- 
  Y |> 
  mutate(diff_ue = ((diff_ue * -1)+100))

Z <- df_final[c("dist_prov_cap", "per_capita_CCAA", "per_capita_n_firms")]

# Mean normalization function
mean_normalize <- function(x) {
  return((x - mean(x)) / (max(x) - min(x))+1)
}

# Apply mean normalization to each column
X <- X |>  mutate(across(everything(), mean_normalize))

Z <- Z |>  mutate(across(everything(), mean_normalize))

Y <- Y |> mutate(across(everything(), mean_normalize))
```

#### Validating inverse assumption

```{r}
dea_naive <-
  dea(XREF = X, YREF = Y, X = X, Y = Y, W =NULL, RTS ="variable", model = "output")

df_dea_naive <-
  df_final |> 
  mutate(dea_naive = dea_naive$thetaOpt)
```


```{r}
df_dea_naive |> 
  arrange(desc(dea_naive)) |> 
  select(NOMBRE, CCAA, party_ccaa, party_mun, per_capita_MUN, dist_prov_cap, per_capita_n_firms, diff_ue, median_inc_con_unit, per_capita_CCAA, dea_naive) |> 
  head()
```

#### Environmental DEA estimation

Introducing data frames into Simar and Wilson (2007) model.

```{r}
dea_robust <- 
  dea.env.robust(X, Y, W=NULL, Z, model="output", RTS="variable",
                L1=10, L2=100, alpha=0.05)
```

Calculating the relative efficiency score, as efficiency scores are only provided in absolute terms.

```{r}
sorted_scores <- sort(dea_robust$delta_hat_hat, decreasing = TRUE)
best_score <- max(sorted_scores)
delta_hat_hat <- dea_robust$delta_hat_hat

df_dea <- 
  df_final |> 
  mutate(dea_robust = delta_hat_hat/best_score)
```

## Descriptive Analysis

### Efficiency score

#### Output II - Difference

```{r}
mean(df_dea$diff_ue)
```
On the unemployment rate went down by almost 1%, as backed up by annual data

#### Output II - Median Income per Consumption Unit

```{r}
mean(df_dea$median_inc_con_unit)
```

#### Input - Municipality Expenditure per Capita

```{r}
mean(df_dea$per_capita_MUN)
```


#### Distribution

```{r}
df_dea |> 
  ggplot()+
  geom_histogram(aes(dea_robust), stat = "bin", bins = 160) +
  theme_minimal() +
  labs(title = "Distribution of DEA Efficiency Score",
       y = "Count")
```

#### Most efficient municipality

Finding most efficient municipalities

```{r}
df_dea |> 
  arrange(desc(dea_robust)) |> 
  select(NOMBRE, CCAA, party_ccaa, party_mun, per_capita_MUN, dist_prov_cap, per_capita_n_firms, diff_ue, median_inc_con_unit, per_capita_CCAA, dea_robust) |> 
  head()
```

Finding influence of environmental variables

robust coefficients in the truncated regression of reciprocal of DEA score on environmental variables (after the second loop).

```{r}
dea_robust$beta_hat_hat
```
#### Mean efficiency score

```{r}
mean(df_dea$dea_robust)
```

0.7701055

#### t-test between city sizes

```{r}
greater_20k <- df_dea[df_dea$greater_20k == 1, "dea_robust"]
smaller_20k <- df_dea[df_dea$greater_20k == 0, "dea_robust"]

# Perform the t-test
t_test_result <- t.test(greater_20k, smaller_20k)
t_test_result
```

Smaller municipalities are more efficient, statistically significant.

```{r}
greater_100k <- df_dea[df_dea$pop_total > 100000, "dea_robust"]
smaller_100k <- df_dea[df_dea$pop_total <= 100000, "dea_robust"]

# Perform the t-test
t_test_result <- t.test(greater_100k, smaller_100k)
t_test_result
```

A bit more pronounced if threshold difference between large and small municipality is greater.

#### Map

```{r}
esp_can <- esp_get_country() 
can_prov <- esp_get_can_provinces()
can_box <- esp_get_can_box() 
munic <- esp_get_munic() 

munic <-    
  munic |>  
  rename(INE = LAU_CODE)

df_dea_map <- munic |> 
  left_join(df_dea, by = "INE")  

ggplot(esp_can) +
  geom_sf() +
  geom_sf(data = can_prov) +
  geom_sf(data = can_box) + 
  geom_sf(data = df_dea_map, aes(fill = dea_robust), color = "black", size = .5)+
  scale_fill_gradient(na.value = "grey", low ="#FF0000",high = "#FFFFFF",name = "Efficiency Score") +
  theme_void() +
  labs(title = "Efficiency Score by Municipality (Spain)")
```

Seems to follow the notion that more money does not necessarily produce more efficent labour market outcomes (but often adverse relationship)

North more inefficient than South

### PCA

remove per_capita_revenue

```{r}
df_pca <-
  df_dea |> 
  select_if(is.numeric) |> 
  select(-c(CODAUTO, CPRO, median_inc_con_unit, diff_ue, per_capita_CCAA, dist_prov_cap, n_firms, per_capita_n_firms, per_capita_MUN)) |> 
  drop_na()

set.seed(53456)
pca <- prcomp(df_pca, scale. = T)
```

```{r}
fviz_screeplot(pca, addlabels = TRUE)
```

shows how much information is in each column
only 12% in one column
using the second component 

Filtering less strong rotation values to get better overview

```{r}
# Create a data frame with PCA rotation values
data.frame(variable = rownames(pca$rotation), rotation_value = pca$rotation[, 1]) |>
  filter(rotation_value > 0.15 | rotation_value < -0.15) |> 
  ggplot(aes(x = variable, y = rotation_value)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Barplot of PCA (First Principal Component)",
       x = "Variable", y = "Rotation Value")  +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Filtering less strong rotation values to get better overview
```{r}
data.frame(variable = rownames(pca$rotation), rotation_value = pca$rotation[, 2]) |>
  filter(rotation_value > 0.1 | rotation_value < -0.1) |> 
  ggplot(aes(x = variable, y = rotation_value)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Barplot of PCA (Second Prinicipal Component)",
       x = "Variable", y = "Rotation Value")  +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Higher 

```{r}
data.frame(z1=pca$x[,1],z2=pca$x[,2]) %>% 
  ggplot(aes(z1,z2)) + geom_point(size=0) +
  labs(title="First two principal components (scores)", x="PC1", y="PC2") + #guides(color=guide_legend(title="HDI"))+
  theme_bw() +theme(legend.position="bottom")
```

```{r}
fviz_contrib(pca, choice = "var", axes = 1)
```

DEA does not contribute as much.

```{r}
fviz_contrib(pca, choice = "var", axes = 2)
```

DEA contributes the second most to component number two

### Clustering

```{r}
df_clus <-
  df_dea |> 
  select_if(is.numeric) |> 
  select(-c(CODAUTO, CPRO, diff_ue, median_inc_con_unit, per_capita_CCAA, per_capita_MUN)) |> 
  drop_na()
```

```{r}
fviz_nbclust(scale(df_clus), kmeans, method = 'silhouette', k.max = 40, nstart = 100)
```


```{r}
cluster <- kmeans(df_clus, centers = 2, nstart = 100)
```

Graph

```{r}
# Assuming 'cluster' is your k-means result
centers <- as.data.frame(cluster$centers)
centers$cluster <- factor(1:nrow(centers))  # Add a column for cluster labels

custom_labels <- c(
  general_services = "Share of Spending\nGeneral Services",
  pub_safety_mobility = "Share of Spending\nPublic Safety & Mobility",
  social_services = "Share of Spending\nSocial Services",
  infrastructure = "Share of Spending\nInfrastructure",
  employment_promotion = "Share of Spending\nEmployment Promotion",
  dist_prov_cap = "Distance to\nProvincial Capital",
  ue_pct_2020 = "Unemployment\nRate 2020",
  pop_share_retirees = "Share of Retirees",
  greater_20k = "More than 20,000\n inhabitants",
  share_contracts_serv = "Share of Contracts\nin Service Sector",
  revenue_tax_share = "Municipal\nShare of Revenue:\nTax",
  pop_density = "Population Density",
  dea_robust = "Efficiency Score",
  pop_share_foreigners = "Share of Foreigners",
  share_essential_employment = "Share of\nEssential Employment",
  share_firms_service = "Share of Firms\nin Service Sector"
)


centers |> 
  select(general_services, pub_safety_mobility, social_services, infrastructure, employment_promotion, dist_prov_cap, ue_pct_2020, pop_share_retirees, greater_20k, share_contracts_serv, revenue_tax_share, pop_density, dea_robust, cluster, pop_share_foreigners, share_essential_employment, share_firms_service) |>
  pivot_longer(-cluster, names_to = "variable", values_to = "value") |>
  ggplot(aes(x = variable, y = value, fill = cluster)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  facet_wrap(~ variable, scales = "free", labeller = labeller(variable = custom_labels)) +
  labs(title = "Cluster Centers", y = "Value", fill = "Cluster") +
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        legend.position =  "bottom")
```

### Correlation

Spearman because most of the data is not normally distributed and relationships are not linear

```{r}
df_corr <-
  df_dea |> 
  select(-c(INE, NOMBRE, CPRO, CODAUTO, dea_robust, diff_ue, median_inc_con_unit, per_capita_MUN, per_capita_n_firms, per_capita_CCAA, dist_prov_cap)) |>
  select_if(is.numeric)

corr<- cor(df_dea$dea_robust, df_corr, method = "spearman", use = "complete.obs")
corr<-round(corr,4)

df_corr_results <- 
  data.frame(corr) |> 
  pivot_longer(values_to = "correlation",
               cols = 1:66,
               names_to = "variable")

ggplot(df_corr_results, aes(x = variable, y = correlation)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Correlation with DEA efficiency score", x = "Variable", y = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 0.5)) 
```

# DELETE
### Employment promotion

```{r}
df_dea |> 
  ggplot()+
  geom_histogram(aes(employment_promotion), stat = "bin", bins = 160) +
  theme_minimal()
```

```{r}
df_dea |> 
  ggplot(aes(employment_promotion, dea_robust))+
  geom_point()
```

### Essential Sectors during Covid-19

```{r}
df_dea |> 
  ggplot()+
  geom_histogram(aes(share_essential_business), stat = "bin", bins = 160) +
  theme_minimal()
```

```{r}
df_dea |> 
  ggplot(aes(share_essential_business, dea_robust))+
  geom_point()
```

```{r}
df_dea |> 
  ggplot()+
  geom_histogram(aes(share_essential_employment), stat = "bin", bins = 160) +
  theme_minimal()
```


```{r}
df_dea |> 
  ggplot(aes(share_essential_business, dea_robust))+
  geom_point()
```

### Creating Employment

```{r}
df_dea |> 
  ggplot()+
  geom_histogram(aes(share_ue_agriculture), stat = "bin", bins = 160) +
  theme_minimal()
```

```{r}
df_dea |> 
  ggplot(aes(share_ue_agriculture, dea_robust))+
  geom_point()
```

### Revenue generation

```{r}
df_dea |> 
  ggplot(aes(social_services, dea_robust))+
  geom_point()
```
# DELETE

## Deleting unnsecessary variables

All variables without additional meaning and variables used in estimating efficiency score+

No revenue data - resembles spending data

```{r}
df_model <- 
  df_dea |> 
  select(-c(CODAUTO, dist_prov_cap, per_capita_CCAA, per_capita_MUN, per_capita_n_firms, median_inc_con_unit, party_mun, party_ccaa, diff_ue))
```

## Data imputation

```{r}
m <- 5
set.seed(475)
imp <- mice(df_model, m = m, method= "rf")
df_model_imp <- complete(imp, action=m)
```

## Preliminary


```{r}
df_model_robust <- df_model_imp |> 
  select(-c(INE, NOMBRE, CCAA, CPRO)) 
```

excluded:
|> 
  filter(!is.na(party_mun)) |> 
  drop_na()
  

## Modelling - Target normal - Random forest

### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train(dea_robust ~ .,
                     data = df_model_robust,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "party")&!startsWith(non_zero_vars, "greater")]


df_model_final <- 
  df_model_imp |>
  dplyr::select(all_of(non_zero_vars), dea_robust, greater_20k, INE, NOMBRE) |> 
  drop_na()
# excluded , party_mun,party_ccaa for the moment

# all_of(non_zero_vars), dea_robust, greater_20k, party_mun, party_ccaa, INE, NOMBRE
```

#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_model_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data  <- df_model_final[training.samples, ]
test.data <- df_model_final[-training.samples, ] 

# deleting INE and NOMBRE from train set to keep in test only
train.data$INE <- NULL
train.data$NOMBRE <- NULL

```

#### Training RF
```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune <- train(dea_robust ~., 
                 data = train.data,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(15,16,17,18)),
                 importance = TRUE)

rf_tune$finalModel$importance

rf_tune$results

varImp(rf_tune)

plot(varImp(rf_tune))
```

##### Results

```{r}
test_results <- data.frame(dea_robust = test.data$dea_robust)

#normal
test_results$rf <- predict(rf_tune, test.data)
rf_normal <- postResample(pred = test_results$rf,  obs = test_results$dea_robust)
```

See data chosen I
0.6485436 
MAE
0.03261175

see data chosen I
Normal
 RMSE   Rsquared        MAE 
0.04348254 0.63551313 0.03342641 

Data chosen II
0.6504064
MAE
0.03264785

see data chosen II
Normal
      RMSE   Rsquared        MAE 
0.04340711 0.63832142 0.03360933 

Data chosen III
0.6552379
MAE
0.03280619
    RMSE   Rsquared        MAE 
0.04294293 0.64908921 0.03300969

Data chosen IV
0.6574947
MAE
0.03240815
   RMSE   Rsquared        MAE 
0.04262793 0.65125692 0.03257174 


--\> constant problem: overfitting (reduced with additional data)

-   **MAE Interpretation:** MAE (Mean Absolute Error) represents the average difference between the predicted values and the actual values. In your case, the average difference is only 0.034, which is a very small portion of the target variable's range (0.5 to 1).

-   **Relative Error:** Considering the limited range of the target variable (from 0.5 to 1), an absolute error of 0.034 is even more significant. It translates to a maximum relative error of around 6.8% (0.034 / (1-0.5) ).

**General Rule of Thumb:**

-   A lower MAE indicates better model performance. In general, an MAE less than 10% of the variable's range is considered good. Here, your MAE is well below that threshold.

Prediction

## Modelling - Target log - Random forest

### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train(log(dea_robust) ~ .,
                     data = df_model_robust,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```
#### Data chosen I
Did not do first two correctly

 [1] "party_munPP"                  "party_munOTROS"               "party_munPSOE"                "party_munN.ADS."              "party_munUCIN"               
 [6] "party_munBNG"                 "party_munPAR"                 "party_munUPL"                 "party_munAHORA DECIDE"        "party_ccaaERC"               
[11] "party_ccaaPNV"                "party_ccaaPSOE"               "general_services"             "environment"                  "social_services"             
[16] "culture"                      "agr_farm_fish"                "tax_fin_admin"                "public_debt"                  "health"                      
[21] "education"                    "infrastructure"               "employment_promotion"         "sports"                       "trans_public_agencies"       
[26] "pub_transportation"           "pensions"                     "r_and_d"                      "area_sqkm"                    "ue_pct_2020"                 
[31] "share_ue_construction"        "share_ue_industry"            "share_ue_agriculture"         "share_ue_wo_job"              "pop_share_foreigners"        
[36] "pop_share_wk_age"             "pop_share_retirees"           "share_women"                  "share_essential_employment"   "share_firms_service"         
[41] "share_contracts_agri"         "share_contracts_con"          "rural_prop_tax"               "special_prop_tax"             "min_coef_turn_tax"           
[46] "revenue_tax_share"            "share_ccaa_general_services"  "share_ccaa_economic"          "share_ccaa_culture"           "share_ccaa_social_protection"
[51] "pop_density"                  "per_capita_contracts"

#### Data chosen II - ideology score
 [1] "general_services"             "environment"                  "social_services"              "agr_farm_fish"                "tax_fin_admin"               
 [6] "public_debt"                  "health"                       "education"                    "infrastructure"               "employment_promotion"        
[11] "sports"                       "trans_public_agencies"        "other_econ"                   "pub_transportation"           "pensions"                    
[16] "r_and_d"                      "area_sqkm"                    "ue_pct_2020"                  "share_ue_service"             "share_ue_construction"       
[21] "share_ue_industry"            "share_ue_agriculture"         "pop_share_foreigners"         "pop_share_wk_age"             "pop_share_retirees"          
[26] "share_women"                  "share_essential_employment"   "share_firms_service"          "share_contracts_agri"         "share_contracts_con"         
[31] "urban_prop_tax"               "rural_prop_tax"               "special_prop_tax"             "min_coef_turn_tax"            "revenue_tax_share"           
[36] "revenue_grants_share"         "share_ccaa_general_services"  "share_ccaa_security"          "share_ccaa_economic"          "share_ccaa_housing_services" 
[41] "share_ccaa_culture"           "share_ccaa_social_protection" "pop_density"                  "per_capita_contracts"         "ideology_score" 

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "party")&!startsWith(non_zero_vars, "greater")]


df_model_final <- 
  df_model_imp |>
  dplyr::select(all_of(non_zero_vars), dea_robust, INE, NOMBRE)|>
  drop_na()
# excluded , party_mun,party_ccaa for the moment
```

filter(!is.na(party_mun)) |> 


#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_model_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data  <- df_model_final[training.samples, ]
test.data <- df_model_final[-training.samples, ] 

# deleting INE and NOMBRE from train set to keep in test only
train.data$INE <- NULL
train.data$NOMBRE <- NULL

```

#### Training RF
```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune_log <- train(log(dea_robust) ~., 
                 data = train.data,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(9,12,15,18,21)),
                 importance = TRUE)

rf_tune_log$finalModel$importance

rf_tune_log$results

varImp(rf_tune_log)

plot(varImp(rf_tune_log))
```

##### Results

```{r}
# log
test_results$rf_log <- predict(rf_tune_log, test.data)
test_results$rf_log <- exp(test_results$rf_log)
rf_log <- postResample(pred = test_results$rf_log,  obs = test_results$dea_robust)
```
see data chosen I
0.6648428
MAE
0.04230489
    RMSE  Rsquared       MAE 
1.0353495 0.6472887 1.0344253

data chosen II
0.6637780
MAE
0.04170904
 RMSE   Rsquared        MAE 
0.04204950 0.66050201 0.03215307 

## Modelling - Target log10 - Random forest
### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train(log10(dea_robust) ~ .,
                     data = df_model_robust,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```
#### Data chosen I
Did not do first two correctly

[1] "party_munPSOE"                "party_munPAR"                 "party_ccaaERC"                "party_ccaaPNV"                "party_ccaaPSOE"              
 [6] "environment"                  "social_services"              "agr_farm_fish"                "tax_fin_admin"                "public_debt"                 
[11] "health"                       "infrastructure"               "employment_promotion"         "pub_transportation"           "area_sqkm"                   
[16] "ue_pct_2020"                  "share_ue_construction"        "share_ue_industry"            "share_ue_agriculture"         "pop_share_foreigners"        
[21] "pop_share_wk_age"             "pop_share_retirees"           "share_women"                  "share_essential_employment"   "share_firms_service"         
[26] "share_contracts_agri"         "share_contracts_con"          "rural_prop_tax"               "special_prop_tax"             "revenue_tax_share"           
[31] "revenue_grants_share"         "share_ccaa_general_services"  "share_ccaa_security"          "share_ccaa_economic"          "share_ccaa_culture"          
[36] "share_ccaa_social_protection" "pop_density"                  "per_capita_contracts" 

data chosen II
[1] "environment"                 "social_services"             "agr_farm_fish"               "tax_fin_admin"               "public_debt"                
 [6] "health"                      "infrastructure"              "employment_promotion"        "pub_transportation"          "area_sqkm"                  
[11] "ue_pct_2020"                 "share_ue_construction"       "share_ue_industry"           "share_ue_agriculture"        "pop_share_foreigners"       
[16] "pop_share_wk_age"            "pop_share_retirees"          "share_women"                 "share_essential_employment"  "share_firms_service"        
[21] "share_contracts_agri"        "share_contracts_con"         "rural_prop_tax"              "special_prop_tax"            "revenue_tax_share"          
[26] "revenue_grants_share"        "share_ccaa_general_services" "share_ccaa_security"         "share_ccaa_economic"         "share_ccaa_housing_services"
[31] "share_ccaa_culture"          "pop_density"                 "per_capita_contracts"

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "party")&!startsWith(non_zero_vars, "greater")]


df_model_final <- 
  df_model_imp |>
  dplyr::select(all_of(non_zero_vars), dea_robust, INE, NOMBRE)|> 
  drop_na()
# excluded party ccaa for the moment
```

Hypothesis: covid employment data makes models overfit --\> too accurate AND decreases R-squared. How?

|> 
  filter(!is.na(party_mun)) 

#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_model_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data  <- df_model_final[training.samples, ]
test.data <- df_model_final[-training.samples, ] 

# deleting INE and NOMBRE from train set to keep in test only
train.data$INE <- NULL
train.data$NOMBRE <- NULL

```

#### Training RF
```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune_log10 <- train(log10(dea_robust) ~., 
                 data = train.data,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(9,12,15,18,21)),
                 importance = TRUE)

rf_tune_log10$finalModel$importance

rf_tune_log10$results

varImp(rf_tune_log10)

plot(varImp(rf_tune_log10))
```

##### Results

```{r}
# log10
test_results$rf_log10 <- predict(rf_tune_log10, test.data)
test_results$rf_log10 <- 10^(test_results$rf_log10)
rf_log10 <- postResample(pred = test_results$rf_log10,  obs = test_results$dea_robust)
```

0.6638687
0.01834957
  RMSE  Rsquared       MAE 
0.8862600 0.6474117 0.8848043 

data chosen II - ideology score
0.6631807
MAE
0.01817255
RMSE   Rsquared        MAE 
0.04228648 0.65581593 0.03220443 

## Target cube root - Random forest

### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train((dea_robust)^(1/3) ~ .,
                     data = df_model_robust,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```
#### Data chosen I
Did not do first two correctly

  [1] "party_munPSOE"                "party_ccaaERC"                "party_ccaaPNV"                "party_ccaaPSOE"               "social_services"             
 [6] "tax_fin_admin"                "public_debt"                  "health"                       "infrastructure"               "employment_promotion"        
[11] "area_sqkm"                    "ue_pct_2020"                  "share_ue_construction"        "share_ue_industry"            "share_ue_agriculture"        
[16] "pop_share_foreigners"         "pop_share_wk_age"             "pop_share_retirees"           "share_essential_employment"   "share_contracts_agri"        
[21] "share_contracts_con"          "rural_prop_tax"               "revenue_tax_share"            "revenue_grants_share"         "share_ccaa_general_services" 
[26] "share_ccaa_security"          "share_ccaa_economic"          "share_ccaa_culture"           "share_ccaa_social_protection" "pop_density"                 
[31] "per_capita_contracts" 

data chosen II - ideology score

[1] "social_services"             "tax_fin_admin"               "public_debt"                 "health"                      "infrastructure"             
 [6] "employment_promotion"        "area_sqkm"                   "ue_pct_2020"                 "share_ue_construction"       "share_ue_industry"          
[11] "share_ue_agriculture"        "pop_share_foreigners"        "pop_share_wk_age"            "pop_share_retirees"          "share_women"                
[16] "share_essential_employment"  "share_firms_service"         "share_contracts_agri"        "share_contracts_con"         "rural_prop_tax"             
[21] "special_prop_tax"            "revenue_tax_share"           "revenue_grants_share"        "share_ccaa_general_services" "share_ccaa_security"        
[26] "share_ccaa_economic"         "share_ccaa_culture"          "pop_density"                 "per_capita_contracts" 

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "party")&!startsWith(non_zero_vars, "greater")]


df_model_final <- 
  df_model_imp |>
  dplyr::select(all_of(non_zero_vars), dea_robust, INE, NOMBRE)|> 
  drop_na()
# excluded party ccaa for the moment
```

Hypothesis: covid employment data makes models overfit --\> too accurate AND decreases R-squared. How?


#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_model_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data  <- df_model_final[training.samples, ]
test.data <- df_model_final[-training.samples, ] 

# deleting INE and NOMBRE from train set to keep in test only
train.data$INE <- NULL
train.data$NOMBRE <- NULL

```

#### Training RF
```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune_cube <- train((dea_robust)^(1/3) ~., 
                 data = train.data,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(9,12,15,18,21)),
                 importance = TRUE)

rf_tune_cube$finalModel$importance

rf_tune_cube$results

varImp(rf_tune_cube)

plot(varImp(rf_tune_cube))
```

##### Results
```{r}
# cube
test_results$rf_cube <- predict(rf_tune_cube, test.data)
test_results$rf_cube <- (test_results$rf_cube)^3
rf_cube <- postResample(pred = test_results$rf_cube,  obs = test_results$dea_robust)
```


0.6542624
MAE
0.01300218
     RMSE  Rsquared       MAE 
0.1562569 0.6477807 0.1460189 

data chosen II
0.6619849
MAE
0.01284411
RMSE   Rsquared        MAE 
0.04242569 0.65422809 0.03235297 


#### Compare Results

```{r}
cube_df <- as.data.frame(t(rf_cube))
log_df <- as.data.frame(t(rf_log))
normal_df <- as.data.frame(t(rf_normal))
log10_df <- as.data.frame(t(rf_log10))

cube_df$model <- "Cube Root"
log_df$model <- "Log"
normal_df$model <- "Normal"
log10_df$model <- "Log10"

rf_metrics_table <- rbind(cube_df, log_df, normal_df, log10_df)

rf_metrics_table
```


#### Visualizing best

```{r}
qplot(test_results$rf, test_results$dea_robust) + 
  labs(title="RF Regression Observed VS Predicted", x="Predicted", y="Observed") +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

Problem --> Absolute error

## MAE Investigation

```{r}
# normal
mae_per_observation_rf <- abs(test_results$rf - test_results$dea_robust)
test.data$mae_rf <- mae_per_observation_rf 
```

```{r}
df_mae <- 
  test.data |> 
  select(INE, NOMBRE, mae_rf) |> 
  left_join(df_dea, by = c("INE", "NOMBRE")) |> 
  drop_na()
```

### Correlation Analysis

```{r}
df_corr_mae <-
  df_mae |> 
  select_if(is.numeric) |> 
  select(-mae_rf)

corr_mae<- cor(df_mae$mae_rf, df_corr_mae, method = "spearman")
corr_mae<-round(corr_mae,4)

df_corr_mae_results <- 
  data.frame(corr_mae) |> 
  pivot_longer(values_to = "correlation",
               cols = 1:69,
               names_to = "variable")

ggplot(df_corr_mae_results, aes(x = variable, y = correlation)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Correlation with absolute error", x = "Variable", y = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 0.5)) 
```

#### dea_robust variable

```{r}
df_mae |> 
  ggplot()+
  geom_point(aes(x = dea_robust, y = mae_rf)) +
  theme_minimal()
```

#### Mapping MAE

```{r}
esp_can <- esp_get_country() 
can_prov <- esp_get_can_provinces()
can_box <- esp_get_can_box() 
munic <- esp_get_munic() 

munic <-    
  munic |>  
  rename(INE = LAU_CODE)

df_mae_map <- munic |> 
  left_join(df_mae, by = "INE")  

ggplot(esp_can) +
  geom_sf() +
  geom_sf(data = can_prov) +
  geom_sf(data = can_box) + 
  geom_sf(data = df_mae_map, aes(fill = mae_rf), color = "black", size = .5)+
  scale_fill_viridis_c(na.value = NA) +
  theme_minimal()
```

No spatial relationship

```{r}
df_mae |> 
  ggplot()+
  geom_point(aes(x = area_sqkm, y = mae_rf)) +
  theme_minimal()+
    coord_cartesian(xlim = c(0, 300), ylim = c(0, 0.18))
```

### Linear regression

```{r}
df_mae_lm <-
  na.omit(df_mae) |> 
  select(-c(INE, NOMBRE, CCAA, party_ccaa, party_mun, median_inc_con_unit, target, dea_robust))
model <- lm(mae_rf ~ ., data = df_mae_lm)

# Print summary of the model
summary(model)
```
### Absoulte error by Regions (Province and Autonomous Community)

#### Province

```{r}
CPRO_mae <- df_mae |> 
  group_by(CPRO) |> 
  summarise(mae_rf = mean(mae_rf, na.rm = TRUE))

# Create the graph
ggplot(CPRO_mae, aes(x = CPRO, y = mae_rf)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Mean Absolute Error by Province",
       x = "Regional Area (CPRO)",
       y = "Mean Absolute Error") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Map

```{r}
esp_can <- esp_get_country() 
can_prov <- esp_get_can_provinces()
can_box <- esp_get_can_box() 
provs <- esp_get_prov_siane(epsg = 3857)

df_mae_prov <- provs |> 
  left_join(CPRO_mae, by = c("cpro" = "CPRO"))  

ggplot(esp_can) +
  geom_sf() +
  geom_sf(data = can_prov) +
  geom_sf(data = can_box) + 
  geom_sf(data = df_mae_prov, aes(fill = mae_rf), color = "black", size = .5)+
  scale_fill_viridis_c(na.value = NA) +
  theme_minimal()
```

#### Autnomous communities

```{r}
CCAA_mae <- df_mae |> 
  group_by(CCAA, CODAUTO) |> 
  summarise(mae_rf = mean(mae_rf, na.rm = TRUE))

# Create the graph
ggplot(CCAA_mae, aes(x = CCAA, y = mae_rf)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Mean Absolute Error by CCAA",
       x = "Autonomous Community",
       y = "Mean Absolute Error") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Mapping
```{r}
esp_can <- esp_get_country() 
can_prov <- esp_get_can_provinces()
can_box <- esp_get_can_box() 
CCAA_sf <- esp_get_ccaa()

df_mae_ccaa <- CCAA_sf |> 
  left_join(CCAA_mae, by = c("codauto" = "CODAUTO"))  

ggplot(esp_can) +
  geom_sf() +
  geom_sf(data = can_prov) +
  geom_sf(data = can_box) + 
  geom_sf(data = df_mae_ccaa, aes(fill = mae_rf), color = "black", size = .5)+
  scale_fill_viridis_c(na.value = NA) +
  theme_minimal()
```

### Scatter plots
```{r}
ggplot(df_mae) +
    geom_point(aes(x = pop_density, y = mae_rf)) +
    theme_minimal() +
    coord_cartesian(xlim = c(0, 300), ylim = c(0, 0.18))
```

```{r}
ggplot(df_mae) +
    geom_point(aes(x = ue_pct_2020, y = mae_rf)) +
    theme_minimal() +
    coord_cartesian(xlim = c(0, 60), ylim = c(0, 0.18))
```

```{r}
df_mae |> 
  arrange(desc(MAE))
```

## Building the Model

## Feature Engineering

Selecting imputed data frame while keeping CCAA and CPRO

```{r}
df_ml <- 
  df_model_imp |> 
  select(-c(INE, NOMBRE, CCAA)) |> 
  mutate(CPRO = as.factor(CPRO))
```

Adding features

bivariat:

pop_density --\> threshold: 50

area_sqkm --\> trheshold: 50

pop_total --\> threshold: 500

keep CPRO

Interactions:

All of the above with all variables except pca and dea

Function
```{r}
generate_interactions <- function(df) {
  # Extracting numeric columns excluding dea_robust
  numeric_cols <- df %>%
    select(where(is.numeric), -dea_robust)
  
  # Extracting pop_total column
  pop_total_col <- numeric_cols %>%
    select(pop_total, pop_density, area_sqkm)
  
  # Other numeric columns excluding pop_total
  other_numeric_cols <- numeric_cols %>%
    select(-c(pop_total, pop_density, area_sqkm))
  
  # Generating interactions
  interaction_cols <- list()
  
  for(col_name in names(other_numeric_cols)) {
    interaction_name <- paste("pop_total", col_name, sep = "_x_")
    interaction_name <- make.names(interaction_name, unique = TRUE)  # Ensure unique column names
    interaction_cols[[interaction_name]] <- pop_total_col[[1]] * other_numeric_cols[[col_name]]
  }
  
  # Combining original and interaction columns
  df_interactions <- cbind(df, as.data.frame(interaction_cols))
  
  return(df_interactions)
}
```

```{r}
# to apply feature engineering
vars_to_engineer <- 
  df_ml |> 
  select(where(is.numeric)) |> 
  names()

# feature engineering
df_ml <- 
  df_ml |> 
  mutate(pop_density_low = as.factor(if_else(pop_density < 50, 1,0)), # adding varaibles
         pop_total_low = as.factor(if_else(pop_total < 500, 1,0)),
         area_sqkm_low = as.factor(if_else(area_sqkm < 50, 1,0))) |>
  generate_interactions() |> 
  mutate(across(all_of(vars_to_engineer), ~log(. + 1), .names = "{.col}_log"), # feature engineering mathematical alteration of variables
         across(all_of(vars_to_engineer), ~.^2, .names = "{.col}_2"),
         across(all_of(vars_to_engineer), ~.^3, .names = "{.col}_3")) |> 
  mutate(pop_total_x_area_sqkm = area_sqkm*pop_total,
         pop_density_X_area_sqkm = pop_density * area_sqkm,
         pop_total_x_pop_density = pop_total*pop_density) |> 
  drop_na()

df_ml$dea_robust_2 <- NULL
df_ml$dea_robust_log <- NULL
df_ml$dea_robust_3 <- NULL
```

### FINALIZE Visualizing remaining municipalities

Where is the difference

```{r}
m <-
  df_spen_imp_mun_tidy |>
  mutate(CODAUTO_data = CODAUTO) |> 
  group_by(CODAUTO_data) |> 
  distinct(INE) |> 
  count(CODAUTO_data)
# adding missing community 
new_row <- data.frame(CODAUTO_data = "19", n = 0)
m <- m |> 
  bind_rows(new_row)

o <- 
  codigo_municipio |>
  mutate(CODAUTO_original = CODAUTO) |> 
  group_by(CODAUTO_original) |> 
  distinct(INE) |> 
  count(CODAUTO_original)

difference_n_mun <- o |> 
  left_join(m, by = c("CODAUTO_original"= "CODAUTO_data")) |> 
  mutate(difference = n.x - n.y,
         proportion = (difference/n.x)*100)

plot1 <-
  difference_n_mun |> 
  ggplot()+
  geom_col(aes(CODAUTO_original, proportion)) +
  geom_text(aes(CODAUTO_original, proportion, label = paste(round(proportion, 2),"%")),
            vjust = -0.4,
            size = 2.5) +
  labs(title = "Share of missing Municipalities by Autonomous Region") +
  theme_minimal()

plot1
```

## Feature selection

### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train(dea_robust ~ .,
                     data = df_ml,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```

#### Data chosen
[1] "CPRO5"                             "CPRO6"                             "CPRO9"                             "CPRO10"                           
 [5] "CPRO11"                            "CPRO12"                            "CPRO13"                            "CPRO14"                           
 [9] "CPRO15"                            "CPRO16"                            "CPRO18"                            "CPRO19"                           
[13] "CPRO21"                            "CPRO22"                            "CPRO23"                            "CPRO24"                           
[17] "CPRO29"                            "CPRO31"                            "CPRO33"                            "CPRO34"                           
[21] "CPRO35"                            "CPRO37"                            "CPRO38"                            "CPRO39"                           
[25] "CPRO40"                            "CPRO41"                            "CPRO42"                            "CPRO43"                           
[29] "CPRO45"                            "CPRO47"                            "CPRO48"                            "social_services"                  
[33] "tax_fin_admin"                     "health"                            "employment_promotion"              "pensions"                         
[37] "share_ue_industry"                 "pop_share_foreigners"              "share_contracts_agri"              "share_contracts_con"              
[41] "share_ccaa_general_services"       "pop_density_low1"                  "area_sqkm_low1"                    "pop_total_x_gov_bodies"           
[45] "pop_total_x_trans_public_agencies" "pop_total_x_pop_share_foreigners"  "pop_total_x_share_ccaa_economic"   "CPRO_log"                         
[49] "environment_log"                   "social_services_log"               "public_debt_log"                   "gov_bodies_log"                   
[53] "pub_transportation_log"            "ue_pct_2020_log"                   "share_ue_construction_log"         "share_ue_industry_log"            
[57] "share_ue_agriculture_log"          "share_ue_wo_job_log"               "pop_share_wk_age_log"              "pop_share_youth_log"              
[61] "share_firms_service_log"           "share_contracts_con_log"           "urban_prop_tax_log"                "revenue_tax_share_log"            
[65] "revenue_other_share_log"           "share_ccaa_economic_log"           "pop_density_log"                   "per_capita_contracts_log"         
[69] "general_services_2"                "health_2"                          "infrastructure_2"                  "rural_prop_tax_2"                 
[73] "share_ccaa_general_services_2"     "per_capita_contracts_2"            "general_services_3"                "housing_urban_plan_3"             
[77] "area_sqkm_3"                       "share_ue_industry_3"               "pop_share_foreigners_3"            "pop_share_youth_3"                
[81] "share_firms_service_3"             "share_contracts_serv_3"            "revenue_grants_share_3"            "share_ccaa_security_3"            
[85] "share_ccaa_social_protection_3"   

II

[1] "CPRO03"                        "CPRO04"                        "CPRO05"                        "CPRO06"                        "CPRO09"                       
 [6] "CPRO10"                        "CPRO11"                        "CPRO12"                        "CPRO13"                        "CPRO14"                       
[11] "CPRO16"                        "CPRO18"                        "CPRO19"                        "CPRO20"                        "CPRO21"                       
[16] "CPRO22"                        "CPRO23"                        "CPRO24"                        "CPRO26"                        "CPRO28"                       
[21] "CPRO29"                        "CPRO31"                        "CPRO33"                        "CPRO34"                        "CPRO35"                       
[26] "CPRO37"                        "CPRO38"                        "CPRO39"                        "CPRO40"                        "CPRO41"                       
[31] "CPRO42"                        "CPRO43"                        "CPRO45"                        "CPRO46"                        "CPRO47"                       
[36] "CPRO48"                        "CPRO50"                        "social_services"               "tax_fin_admin"                 "health"                       
[41] "employment_promotion"          "pensions"                      "share_ue_industry"             "pop_share_foreigners"          "share_contracts_agri"         
[46] "share_contracts_con"           "urban_prop_tax"                "share_ccaa_general_services"   "pop_density_low1"              "area_sqkm_low1"               
[51] "environment_log"               "social_services_log"           "tax_fin_admin_log"             "public_debt_log"               "gov_bodies_log"               
[56] "business_turism_sme_log"       "pub_transportation_log"        "ue_pct_2020_log"               "share_ue_construction_log"     "share_ue_industry_log"        
[61] "share_ue_agriculture_log"      "share_ue_wo_job_log"           "pop_share_wk_age_log"          "pop_share_youth_log"           "share_firms_service_log"      
[66] "share_contracts_con_log"       "revenue_other_share_log"       "share_ccaa_economic_log"       "pop_density_log"               "per_capita_contracts_log"     
[71] "general_services_2"            "environment_2"                 "health_2"                      "infrastructure_2"              "rural_prop_tax_2"             
[76] "share_ccaa_general_services_2" "per_capita_contracts_2"        "general_services_3"            "housing_urban_plan_3"          "area_sqkm_3"                  
[81] "share_ue_industry_3"           "pop_share_youth_3"             "share_firms_service_3"         "share_contracts_serv_3"        "rural_prop_tax_3"             
[86] "revenue_grants_share_3"        "share_ccaa_security_3



```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "CPRO")&!startsWith(non_zero_vars, "greater")&!endsWith(non_zero_vars, "low1")]

df_ml_final <- 
  df_ml |> 
  dplyr::select(all_of(non_zero_vars), dea_robust, CPRO, area_sqkm_low, pop_density_low, greater_20k, pop_total_x_pop_density, pop_total_x_area_sqkm, pop_density_X_area_sqkm)
```

Keeping greater_20k beause of previous insights about 


#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_ml_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data  <- df_ml_final[training.samples, ]
test.data <- df_ml_final[-training.samples, ] 
```

Checking distributions

```{r}
train.data |> 
  ggplot()+
  geom_histogram(aes(dea_robust), stat = "bin", bins = 160) +
  theme_minimal() +
  labs(title = "Distribution of DEA Efficiency Score",
       y = "Count")
```

```{r}
test.data |> 
  ggplot()+
  geom_histogram(aes(dea_robust), stat = "bin", bins = 160) +
  theme_minimal() +
  labs(title = "Distribution of DEA Efficiency Score",
       y = "Count")
```



### Training Random Forest

```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune_final <- train(dea_robust ~., 
                 data = train.data,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 1000,
                 tuneGrid = data.frame(mtry= c(50:56)), # still need to run
                 importance = TRUE)
```

Show RÂ² of best model to show is not overfitting
```{r}
rf_tune_final$finalModel$importance

summary(rf_tune_final$finalModel)

plot(rf_tune_final)

rf_tune_final_n$bestTune

rf_tune_final_n$results
```

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 55.

```{r}
model_results <- data.frame(dea_robust = test.data$dea_robust)
model_results$rf <- predict(rf_tune_final, test.data)

# to store model performance metrics
performance_metrics_rf <- postResample(pred = model_results$rf, obs = model_results$dea_robust)
performance_metrics_rf
model_performance <- data.frame(performance_metrics_rf)
```
### Training Extreme Gradian Boosting

```{r}
ctrl <- trainControl(method = "cv", number = 5)

tune_grid <- expand.grid(
  nrounds = c(1500),
  max_depth = c(10),
  eta = c(0.005, 0.001),
  gamma = c(0.000001),
  colsample_bytree = c(0.8),
  min_child_weight = c(5),
  subsample = c(0.7)
)

set.seed(123)
xgb_tune <- train(dea_robust ~., 
                  data = train.data,
                  method = "xgbTree",
                  preProc=c('scale','center'),
                  trControl = ctrl,
                  tuneGrid = tune_grid)
```

```{r}
model_results$xg_boost <- predict(xgb_tune, test.data)
performance_metrics_xgb <- postResample(pred = model_results$xg_boost,  obs = model_results$dea_robust)
performance_metrics_xgb
model_performance <- cbind(data.frame(model_performance, performance_metrics_xgb))
```


```{r}
xgb_tune$results

varImp(xgb_tune)
```

Results
eta   max_depth gamma colsample_bytree min_child_weight subsample nrounds RMSE Rsquared   MAE
0.005	10	0.001	0.8	5	0.7	1500	0.03982445	0.6859576   0.03032696

### Catboost

```{r}
ctrl <- trainControl(method = "cv", number = 5)
```

```{r}
# Specify the features and the target
features <- names(df_ml_final)[names(df_ml_final) != "dea_robust"]
target <- "dea_robust"

# Convert data to catboost pool
train_pool <- catboost.load_pool(data = train.data[, features], label = train.data[[target]])
test_pool <- catboost.load_pool(data = test.data[, features],label = test.data[[target]])

fit_params <- list(iterations = 300,
                   learning_rate = 0.1,
                   depth = 10,
                   loss_function = 'RMSE',
                   l2_leaf_reg = 20,
                   use_best_model = TRUE,
                   od_type = "Iter")

# Train the CatBoost model
catboost_tune <- catboost.train(train_pool, params = fit_params, test_pool = test_pool)
```
Performance on test set

```{r}
# Make predictions on the test data
pred <- catboost.predict(catboost_tune, test_pool)
model_results$catboost <- pred

# storing it in model performance table
performance_metrics_cat <- postResample(pred = model_results$catboost, obs = model_results$dea_robust)
performance_metrics_cat
model_performance <- cbind(data.frame(model_performance, performance_metrics_cat))
```

### Model selection

```{r}
model_performance
```

## Interpretation

### Variable importance

Creating my own plot for consistency

```{r}
var_imp <- varImp(xgb_tune)

# Convert the variable importance data to a data frame
var_imp_df <- as.data.frame(var_imp$importance)
var_imp_df$Variables <- rownames(var_imp_df)

unique(var_imp_df$Variables)


# renamed labels of first thirty variables
new_labels <- c(
  "share_ccaa_security_3" = "Share of Security Spending in CCAA (^3)",
  "share_ccaa_general_services" = "Share of General Services Spending in CCAA",
  "share_ue_agriculture_log" = "Share of Unemployed from Agriculture Sector (Log)",
  "pop_share_foreigners" = "Share of Foreigners in Population",
  "ue_pct_2020_log" = "Unemployment Percentage 2020 (Log)",
  "pop_share_wk_age_log" = "Share of Working Age Population (Log)",
  "pop_share_youth_log" = "Share of Youth (Log)",
  "share_contracts_agri" = "Share of New Contracts in Agriculture Sector",
  "per_capita_contracts_log" = "Average New Contracts per Capita and Month (Log)",
  "pop_density_log" = "Population Density (Log)",
  "revenue_grants_share_3" = "Share of Revenue from Grants (^3)",
  "share_ccaa_economic_log" = "Share of Economic Spending in CCAA (Log)",
  "share_ccaa_general_services_2" = "Share of General Services Spending in CCAA (Log)",
  "pop_total_x_pop_density" = "Population Total x Density",
  "employment_promotion" = "Share of Employment Promotion Spending",
  "share_ue_industry" = "Share of Unemployment from Industry Sector",
  "share_ue_construction_log" = "Share of Unemployment from Construction Sector (Log)",
  "general_services_2" = "Share of General Services Spending (^2)",
  "share_firms_service_log" = "Share of Firms in Service Sector (Log)",
  "share_contracts_serv_3" = "Share of New Contracts per Capita and Month in Service Sector",
  "pop_density_X_area_sqkm" = "Population Density x Area (in SQKM)",
  "share_ue_wo_job_log" = "Share of Unemployed without a Prior Job (Log)",
  "housing_urban_plan_3" = "Share of Housing and Urban Planning Spending (^3)",
  "revenue_other_share_log" = "Share of Revenue from Different Sources (Log)",
  "area_sqkm_3" = "Area in SQKM (^3)",
  "social_services" = "Share of Social Services Spending",
  "pop_total_x_area_sqkm" = "Population Total x Area (in SQKM)",
  "gov_bodies_log" = "Government Bodies (Log)",
  "environment_log" = "Share of Environment Spending (Log)"
)

# Plot showing first thirty variables
var_imp_df |>
  filter(Overall > 3.25) |> 
  ggplot(aes(x = reorder(Variables, Overall), y = Overall)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance Extreme Gradiant Boosting Model", x = "", y = "Variable Importance") +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 5.3))+ 
  scale_x_discrete(labels = new_labels)
```

```{r}
ggplot(df_ml_final) +
    geom_point(aes(x = revenue_grants_share_3, y = pop_total_x_area_sqkm)) +
    theme_minimal() 
```


### PDP

####Policy

```{r}
# employment promotion
partial_plot_emp <- partial(xgb_tune, pred.var = "employment_promotion", plot = FALSE, rug = TRUE, train = train.data)
# Graph
ggplot(partial_plot_emp, aes(x = employment_promotion, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(title = "Partial Dependence Plot for Employment Promotion",
       x = "Employment Promotion",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank()) + 
  scale_x_continuous(breaks = seq(0, max(partial_df$employment_promotion), by = 5), minor_breaks = NULL)

# General services
partial_plot_gen <- partial(xgb_tune, pred.var = "general_services_2", plot = FALSE, rug = TRUE, train = train.data)
# Graph
ggplot(partial_plot_gen, aes(x = general_services_2, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(title = "Partial Dependence Plot for General Services",
       x = "General Services",
       y = "Predicted Efficiency Score") +
  theme_minimal()

social_services
general_services_2
# General services
partial_plot_soc <- partial(xgb_tune, pred.var = "social_services", plot = FALSE, rug = TRUE, train = train.data)
# Graph
ggplot(partial_plot_soc, aes(x = social_services, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(title = "Partial Dependence Plot for Social Services",
       x = "Social Services",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank())

# Environmental spending
partial_plot_env <- partial(xgb_tune, pred.var = "environment_log", plot = FALSE, rug = TRUE, train = train.data)
# Graph
ggplot(partial_plot_env, aes(x = environment_log, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(title = "Partial Dependence Plot for Environment Spending",
       x = "Environment Spending",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank()) 




# use patchwork


```


#### Structural

```{r}
# employment promotion
partial_plot_sec_ccaa <- partial(xgb_tune, pred.var = "share_ccaa_security_3", plot = FALSE, rug = TRUE, train = train.data)
# Graph
ggplot(partial_plot_sec_ccaa, aes(x = share_ccaa_security_3, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(title = "Partial Dependence Plot for CCAA Security Spending",
       x = "Cubed CCAA Security Spending (Share)",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank())

# General services
partial_plot_gen_ccaa <- partial(xgb_tune, pred.var = "share_ccaa_general_services", plot = FALSE, rug = TRUE, train = train.data)
# Graph
ggplot(partial_plot_gen_ccaa, aes(x = share_ccaa_general_services, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(title = "Partial Dependence Plot for CCAA General Services Spending",
       x = "CCAA General Services Share",
       y = "Predicted Efficiency Score") +
  theme_minimal()

# General services
partial_plot_foreigners <- partial(xgb_tune, pred.var = "pop_share_foreigners", plot = FALSE, rug = TRUE, train = train.data)
# Graph
ggplot(partial_plot_foreigners, aes(x = pop_share_foreigners, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(title = "Partial Dependence Plot for Share of Foreigners",
       x = "Share of Foreigners",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank())

# Environmental spending
partial_plot_ue <- partial(xgb_tune, pred.var = "ue_pct_2020_log", plot = FALSE, rug = TRUE, train = train.data)
# Graph
ggplot(partial_plot_ue, aes(x = ue_pct_2020_log, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(title = "Partial Dependence Plot for Unemployment Percentage 2020",
       x = "Unemployment Percentage 2020 (log)",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank()) 
```
### SHAP

#### Importance Bee Plot

```{r}
X <- train.data[sample(nrow(train.data), 1000), -which(names(train.data) == "dea_robust")]
bg_X <- train.data[sample(nrow(train.data), 100), -which(names(train.data) == "dea_robust")]

s <- kernelshap(xgb_tune, X = X, bg_X = bg_X) 
sv <- shapviz(s)
sv_importance(sv, kind = "bee", max_display = Inf) +
  theme_minimal()
```

#### Dependence Plot - Structural Features

share_ccaa_security_3
share_ccaa_general_services
pop_share_foreigners
ue_pct_2020_log
CPRO
pop_density_log
pop_density_x_

```{r}
SHAP_structural <- c("per_capita_contracts", "pop_density", "pop_share_wk_age", "area_sqkm", "rural_prop_tax", "social_services")
sv_dependence(sv, v = v_select_1)
```


#### Dependence Plots - Policy Features

```{r}
SHAP_policy <- c("employment_promotion", "environment_log","social_services","general_services_2")
sv_dependence(sv, v = SHAP_policy) +
  labs(title = "Custom Plot Title")
```

### Under and overachievers

```{r}

yhat = test_results$rf
hist(yhat, col="lightblue")

y = test_results$dea_robust
error = y-yhat
hist(error, col="lightblue")

noise <- error[1:100]

lwr <- yhat[101:length(yhat)] + quantile(noise,0.05, na.rm=T)
upr <- yhat[101:length(yhat)] + quantile(noise,0.95, na.rm=T)

predictions = data.frame(real=y[101:length(y)], fit=yhat[101:length(yhat)], lwr=lwr, upr=upr)

predictions = predictions %>% mutate(out=factor(if_else(real<lwr | real>upr,1,0)))

mean(predictions$out==1)

ggplot(predictions, aes(x=fit, y=real))+
  geom_point(aes(color=out)) + theme(legend.position="none") +
  geom_ribbon(data=predictions,aes(ymin=lwr,ymax=upr),alpha=0.3) +
  labs(title = "Prediction intervals", x = "Prediction",y="Estimated Efficiency")
```

## Appendix

### Historical Unemployment Rate Development

```{r}
# Read the file line by line
lines <- readLines("Data_2020/world_ue_data.csv", encoding = "UTF-8")

# Extract column names from the third line
column_names <- unlist(strsplit(lines[5], ",", fixed = TRUE))
column_names <- gsub('"', "", column_names)  # Remove quotes
column_names <- gsub("\\\\", "", column_names)

# to shift columns one column to the right
column_names <- c("country_name", column_names)


# Split each line into columns based on commas
data <- lapply(lines[-(1:5)], function(line) {
  unlist(strsplit(line, ",", fixed = TRUE))
})

# Convert the list to a data frame and assign column names
data <- as.data.frame(do.call(rbind, data))
names(data) <- column_names


data |> 
  select("country_name","1990":"2021") |> 
  mutate(across("1990":"2021", ~str_extract(., "\\d+\\.?\\d*"))) |> 
  filter(country_name == '"Spain') |> 
  pivot_longer(cols = "1990":"2021",
               values_to = "ue_rate",
               names_to = "year") |> 
  mutate(ue_rate = as.numeric(ue_rate)/100,
         year = as.Date(paste0(year, "-01-01"))) |> 
  ggplot()+
  geom_line(aes(x = year, y = ue_rate), colour = "steelblue") +
  labs(title = "Unemployment Rate 1990 - 2021 (Spain)",
       y = "",
       x = "") +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_x_date(date_labels = "%Y", breaks = "1 year", expand = c(0, 0)) +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

