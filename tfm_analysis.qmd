---
title: "Analysis"
author: "Frederick Pena Sims"
format: html
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## Analysis

### Library

To verify all the necessary packages are installed, run the following code.

```{r}
check_and_install <- function(package) {
  if (!requireNamespace(package, quietly = TRUE)) {
    install.packages(package)
  }
  library(package, character.only = TRUE)
}

packages <- c(
  "devtools", "tidyverse", "corrplot", "readxl", "rDEA", 
  "caret", "mice", "xgboost", "mapSpain", "kernelshap", "shapviz", 
  "pdp", "factoextra", "patchwork", "sf", "scales", "psych", 
  "officer", "flextable"
)

sapply(packages, check_and_install)
```

To install the CatBoost library, follow the following step.

```{r, eval=FALSE}
install.packages('remotes')
remotes::install_url('https://github.com/catboost/catboost/releases/download/v1.2.5/catboost-R-windows-x86_64-1.2.5.tgz', INSTALL_opts = c("--no-multiarch", "--no-test-load"))
```

To load packages individually, follow the following step.

```{r}
options(scipen = 999)
library(catboost)
library(devtools)
library(tidyverse)
library(corrplot)
library(readxl)
library(rDEA)
library(caret)
library(mice)
library(xgboost)
library(mapSpain)
library(kernelshap)
library(shapviz)
library(pdp)
library(factoextra)
library(patchwork)
library(sf)
library(scales)
library(psych)
library(officer)
library(flextable)
```

### Load data

If needed, load the data into the environment. The repository contains the correct file already.

```{r}
df_final <- read.csv("Data_2020/TFM_data_2024-06-12.csv", colClasses = c(INE = "character", greater_20k = "factor", CODAUTO = "character")) |> 
  select(-X)
```

## DEA

### Isotonocity

To run a DEA, the isotonicity property needs to be considered for each input-output pair individually. For Output 1, the data should not violate the isotonicity assumption. For Output 2, since the correlation is slightly negative but close to zero, the impact might be negligible, and the DEA model can still be constructed and interpreted meaningfully.

```{r}
df_iso <- 
  df_final |> 
  select(c(median_inc_con_unit, diff_ue, per_capita_MUN)) |> 
  mutate(diff_ue = (diff_ue * -1)+100)

corr_iso<- cor(df_iso, method = "pearson")
corrplot(corr_iso)
```

### Efficiency score estimation

Following the documentation of the `rDEA` package, variables are stored in separate objects and subsequently standardized.

The difference between 2021 and 2020 is inversed so negative values (reducing unemployment) are observed as better by DEA while greater differences are made negative. This is done, as the DEA interprets larger values as better. A constant is added to not infringe the non-negative constraint.

```{r}
X <- df_final[c("per_capita_MUN")]

Y <- df_final[c("diff_ue", "median_inc_con_unit")]
Y <- 
  Y |> 
  mutate(diff_ue = ((diff_ue * -1)+100))

Z <- df_final[c("dist_prov_cap", "per_capita_CCAA", "per_capita_n_firms")]

# Mean normalization function
mean_normalize <- function(x) {
  return((x - mean(x)) / (max(x) - min(x))+1)
}

# Apply mean normalization to each column
X <- X |>  mutate(across(everything(), mean_normalize))

Z <- Z |>  mutate(across(everything(), mean_normalize))

Y <- Y |> mutate(across(everything(), mean_normalize))
```

#### Validation DEA

To validate the assumption that the inverted variable on the difference in unemployment between two years is picked up correctly by the DEA, a naive DEA containing only input and output variables is conducted.

```{r}
set.seed(3643)
dea_naive <-
  dea(XREF = X, YREF = Y, X = X, Y = Y, W =NULL, RTS ="variable", model = "output")

df_dea_naive <-
  df_final |> 
  mutate(dea_naive = dea_naive$thetaOpt)
```

```{r}
df_dea_naive |> 
  arrange(desc(dea_naive)) |> 
  select(NOMBRE, CCAA, party_ccaa, party_mun, per_capita_MUN, dist_prov_cap, per_capita_n_firms, diff_ue, median_inc_con_unit, per_capita_CCAA, dea_naive) |> 
  head()
```

The result shows that the most efficient municipalities all coincide in having substantial unemployment rate reductions along with high median income values per consumption unit, while showing minimal expenditure.

#### Environmental DEA estimation

Now, environmental data is introduced to estimate efficiency scores using the Simar and Wilson (2007) model.

```{r}
dea_robust <- 
  dea.env.robust(X, Y, W=NULL, Z, model="output", RTS="variable",
                L1=10, L2=100, alpha=0.05)
```

Since efficiency scores are only provided in absolute terms, relative efficiency scores are calculated manually.

```{r}
sorted_scores <- sort(dea_robust$delta_hat_hat, decreasing = TRUE)
best_score <- max(sorted_scores)
delta_hat_hat <- dea_robust$delta_hat_hat

df_dea <- 
  df_final |> 
  mutate(dea_robust = delta_hat_hat/best_score)
```

## Descriptive Analysis

### Efficiency score

#### Distribution

A simple distribution of the subsequent target variable (efficiency score) is calculated.

```{r}
mean_val <- mean(df_dea$dea_robust)
median_val <- median(df_dea$dea_robust)
min_val <- min(df_dea$dea_robust)

Figure_B2 <- 
  df_dea |> 
  ggplot()+
  geom_histogram(aes(dea_robust), stat = "bin", bins = 100, fill = "steelblue") +
  theme_minimal() +
  scale_x_continuous(labels = percent_format(), breaks = seq(0.6, 1, by = 0.05)) + 
  labs(y = "Count",
       x = "Efficiency Score") +
  # Add annotations
  annotate("text", x = mean_val+0.05, y = Inf, label = paste("Mean:", percent(mean_val, accuracy = 0.1)), vjust = 2, color = "black", size = 3) +
  geom_segment(aes(x = mean_val, xend = mean_val+0.05, y = 120, yend = mean_val+125), color = "black") +
  annotate("text", x = median_val-0.07, y = Inf, label = paste("Median:", percent(median_val, accuracy = 0.1)), vjust = 2, color = "black", size = 3) +
  geom_segment(aes(x = median_val, xend = median_val-0.07, y = 120, yend = median_val+125), color = "black") +
  annotate("text", x = min_val+0.01, y = 50, label = paste("Min:", percent(min_val, accuracy = 0.1)), color = "black", size = 3)+
  geom_segment(aes(x = min_val, xend = min_val+0.01, y = min_val, yend = 47), color = "black")

Figure_B2
```

```{r}
ggsave(plot = Figure_B2, "Data_2020/Figures/Figure_B2.png", dpi = 300, bg = "white", width = 10, height = 7)
```

#### Most and least efficient municipalities

To get an ideo of how the efficiency score relates to the input and output variables, the five most efficienct and least efficient municipalities are shown.

```{r}
df_dea |> 
  arrange(desc(dea_robust)) |> 
  select(INE, NOMBRE, CCAA, party_ccaa, party_mun, per_capita_MUN, dist_prov_cap, per_capita_n_firms, diff_ue, median_inc_con_unit, per_capita_CCAA, dea_robust) |> 
  head()
```

```{r}
df_dea |> 
  arrange(desc(dea_robust)) |> 
  select(NOMBRE, CCAA, party_ccaa, party_mun, per_capita_MUN, dist_prov_cap, per_capita_n_firms, diff_ue, median_inc_con_unit, per_capita_CCAA, dea_robust) |> 
  tail()
```

#### Mean efficiency score

```{r}
mean(df_dea$dea_robust)
```

#### Environmental variables

The DEA algorithm also provides information on the coefficients of the truncated regression of reciprocal of DEA score on environmental variables (after the second loop).

```{r}
dea_robust$beta_hat_hat
```

#### t-test between municipality sizes

T-tests are conducted to understand if efficiency scores of larger municipalities are structurally different from efficacy values in smaller municipalities.

```{r}
greater_20k <- df_dea[df_dea$greater_20k == 1, "dea_robust"]
smaller_20k <- df_dea[df_dea$greater_20k == 0, "dea_robust"]

# Perform the t-test
t_test_result <- t.test(greater_20k, smaller_20k)
t_test_result
```

```{r}
greater_100k <- df_dea[df_dea$pop_total > 100000, "dea_robust"]
smaller_100k <- df_dea[df_dea$pop_total <= 100000, "dea_robust"]

# Perform the t-test
t_test_result <- t.test(greater_100k, smaller_100k)
t_test_result
```

#### Mapping the Efficiency score

Using the MapSpain package, the efficiency score can be mapped very easily.

```{r}
esp_can <- esp_get_country() 
can_prov <- esp_get_can_provinces()
can_box <- esp_get_can_box() 
munic <- esp_get_munic() 

munic <-    
  munic |>  
  rename(INE = LAU_CODE)

df_dea_map <- munic |> 
  left_join(df_dea, by = "INE")  

avion_data <- df_dea_map |> 
  filter(name == "Avión")
avion_coords <- st_coordinates(avion_data)


Figure_2 <- 
  ggplot(esp_can) +
  geom_sf() +
  geom_sf(data = can_prov) +
  geom_sf(data = can_box) + 
  geom_sf(data = df_dea_map, aes(fill = dea_robust), color = "black", size = .5)+
  geom_text(data = avion_data, aes(x = avion_coords[1, 1], y = avion_coords[1, 2], label = "Avión"), 
            nudge_x = -1.2, nudge_y = -1.2, size = 3, color = "black") +
  geom_segment(data = avion_data, aes(x = avion_coords[1, 1], y = avion_coords[1, 2], 
                                 xend = avion_coords[1, 1] - 1.2, yend = avion_coords[1, 2] - 1.2), 
               size = 0.5, color = "black") + 
  scale_fill_gradient(na.value = "grey", low ="#FF0000",high = "#FFFFFF",name = "Efficiency\nScore") +
  theme_void()

Figure_2
```

```{r}
ggsave(plot = Figure_2,"Data_2020/Figures/Figure_2.png", dpi = 300, width = 10, height = 7, bg = "white")
```

### PCA

In order to select only valid variables for PCA, a separate data frame is constructed and the PCA subsequently conducted.

```{r}
df_pca <-
  df_dea |> 
  select_if(is.numeric) |> 
  select(-c(CODAUTO, CPRO, median_inc_con_unit, diff_ue, per_capita_CCAA, dist_prov_cap, n_firms, per_capita_n_firms, per_capita_MUN)) |> 
  drop_na()

set.seed(53456)
pca <- prcomp(df_pca, scale. = T)
```

#### PC variance - In Appendix

A screeplot is created and minimally altered.

```{r}
screeplot <- fviz_screeplot(pca, addlabels = TRUE)

Figure_B3 <- 
  screeplot + ggtitle(NULL) +
  theme_minimal()

Figure_B3
```

```{r}
ggsave(plot = Figure_B3, "Data_2020/Figures/Figure_B3.png", dpi = 300, width = 10, height = 7, bg = "white")
```

#### PC1

Efficiency score does not contribute significantly to this Principal Component.

```{r}
fviz_contrib(pca, choice = "var", axes = 1)
```

Less pronounced rotation values are filtered out for better legibility.

```{r}
# Create a data frame with PCA rotation values
data.frame(variable = rownames(pca$rotation), rotation_value = pca$rotation[, 1]) |>
  filter(rotation_value > 0.15 | rotation_value < -0.15) |> 
  ggplot(aes(x = variable, y = rotation_value)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Barplot of PCA (First Principal Component)",
       x = "Variable", y = "Rotation Value")  +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

#### PC2 - in text

Efficiency score contributes significantly to second PC.

```{r}
fviz_contrib(pca, choice = "var", axes = 2)
```

Again, less pronounced rotation values are filtered out for better legibility.

```{r}
Figure_3 <- 
  data.frame(variable = rownames(pca$rotation), rotation_value = pca$rotation[, 2]) |>
  filter(rotation_value > 0.1 | rotation_value < -0.1) |> 
  ggplot(aes(x = variable, y = rotation_value)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(y = "Rotation Value", x= "")  +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_discrete(labels = c("community_wellbeing" = "Com. Wellbeing", 
                              "dea_robust" = "Efficiency Score",
                              "employment_promotion" = "Employment Promotion", 
                              "gov_bodies" = "Government Bodies",
                              "party_mun_id" = "Party Ideology (MUN)", 
                              "dea_robust" = "Efficiency Score",
                              "per_capita_contracts" = "Contracts Per Capita", 
                              "revenue_grants_share" = "Share of Revenue Grants",
                              "revenue_other_share" = "Revenue Other Sources",
                              "revenue_tax_share" = "Share of Revenue Tax", 
                              "rural_prop_tax" = "Rural Property Tax Rate",
                              "share_ccaa_culture" = "Culture (CCAA)", 
                              "share_ccaa_housing_services" = "Housing Services (CCAA)",
                              "share_ccaa_security" = "Security (CCAA)", 
                              "share_ccaa_social_protection" = "Social Protection (CCAA)",
                              "share_contracts_agri" = "Share Contracts Agriculture Sector", 
                              "share_contracts_con" = "Share Contracts Construction Sector",
                              "share_contracts_indu" = "Share Contracts Industrial Sector", 
                              "share_contracts_serv" = "Share Contracts Service Sector",
                              "share_essential_employment" = "Share of Employment Essential", 
                              "share_ue_agriculture" = "Share Unemployment from Agriculture Sector",
                              "share_ue_construction" = "Share Unemployment from Construction Sector", 
                              "share_ue_service" = "Share Unemployment from Service Sector",
                              "social_services" = "Social Services", 
                              "ue_pct_2020" = "Unemployment Rate (2020)"))

Figure_3
```

```{r}
ggsave(plot = Figure_3,"Data_2020/Figures/Figure_3.png", bg ="white", width = 10, height = 7)
```

### Clustering

As for PCA, a data frame for clustering is created.

```{r}
df_clus <-
  df_dea |> 
  select_if(is.numeric) |> 
  select(-c(CODAUTO, CPRO, diff_ue, median_inc_con_unit, per_capita_CCAA, per_capita_MUN)) |> 
  drop_na()
```

#### Optimal Cluster estimation - In Appendix

Using the silhouette method, the optimal number of clusters is estimated.

```{r}
fviz_nbclust <- fviz_nbclust(scale(df_clus), kmeans, method = 'silhouette', k.max = 40, nstart = 100)

Figure_B4 <- fviz_nbclust + ggtitle(NULL) +
  theme_minimal()

Figure_B4
```

```{r}
ggsave(plot = Figure_B4, "Data_2020/Figures/Figure_B4.png", dpi = 300, width = 10, height = 7, bg = "white")
```

```{r}
cluster <- kmeans(df_clus, centers = 2, nstart = 100)
```

#### Centers - in Text

To visualize clusters, centers are taken from cluster object and passed into a `ggplot` object.

```{r}
# Assuming 'cluster' is your k-means result
centers <- as.data.frame(cluster$centers)
centers$cluster <- factor(1:nrow(centers))  # Add a column for cluster labels

custom_labels <- c(
  general_services = "Share of Spending\nGeneral Services",
  pub_safety_mobility = "Share of Spending\nPublic Safety & Mobility",
  social_services = "Share of Spending\nSocial Services",
  infrastructure = "Share of Spending\nInfrastructure",
  employment_promotion = "Share of Spending\nEmployment Promotion",
  dist_prov_cap = "Distance to\nProvincial Capital",
  ue_pct_2020 = "Unemployment\nRate 2020",
  pop_share_retirees = "Share of Retirees",
  share_contracts_serv = "Share of Contracts\nin Service Sector",
  revenue_tax_share = "Municipal\nShare of Revenue:\nTax",
  pop_density = "Population Density",
  dea_robust = "Efficiency Score",
  pop_share_foreigners = "Share of Foreigners",
  share_essential_employment = "Share of\nEssential Employment",
  share_firms_service = "Share of Firms\nin Service Sector",
  housing_urban_plan = "Share of Spending\nHousing and Urban\nPlanning"
)


Figure_4 <- 
  centers |> 
  select(general_services, pub_safety_mobility, social_services, infrastructure, employment_promotion, dist_prov_cap, ue_pct_2020, pop_share_retirees, share_contracts_serv, revenue_tax_share, pop_density, dea_robust, cluster, pop_share_foreigners, share_essential_employment, share_firms_service, housing_urban_plan) |>
  pivot_longer(-cluster, names_to = "variable", values_to = "value") |> # values stored in wide format - transformed into long format for ggplot
  ggplot(aes(x = variable, y = value, fill = cluster)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  facet_wrap(~ variable, scales = "free", labeller = labeller(variable = custom_labels)) +
  labs(y = "Average Value", fill = "Cluster", x = "") + 
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        legend.position =  "bottom")+
  scale_fill_manual(values = c("steelblue", "#FF0000"))

Figure_4
```

```{r}
ggsave(plot = Figure_4,"Data_2020/Figures/Figure_4.png", bg = "white", dpi = 300, width = 10, height = 7)
```

### Correlation - in Text

Spearman correlations with the efficiency score are estimated and passed into a `ggplot` object.

```{r}
df_corr <-
  df_dea |> 
  select(-c(INE, NOMBRE, CPRO, CODAUTO, dea_robust, diff_ue, median_inc_con_unit, per_capita_MUN, per_capita_n_firms, per_capita_CCAA, dist_prov_cap)) |>
  select_if(is.numeric)

corr<- cor(df_dea$dea_robust, df_corr, method = "spearman", use = "complete.obs")
corr<-round(corr,4)

df_corr_results <- 
  data.frame(corr) |> 
  pivot_longer(values_to = "correlation",
               cols = 1:65,
               names_to = "variable")

Figure_5 <- 
  df_corr_results |> 
  filter(correlation > 0.1 | correlation < -0.1) |> 
  ggplot(aes(x = variable, y = correlation)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(y = "Correlation Coefficient", x = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+ 
  scale_x_discrete(labels = c("agr_farm_fish" = "Agriculture",
                              "area_sqkm" = "Area in Square KM",
                              "community_wellbeing" = "Com. Wellbeing", 
                              "employment_promotion" = "Employment Promotion",
                              "environment" = "Environment",
                              "gov_bodies" = "Government Bodies",
                              "ideology_score" = "Government Ideology Score",
                              "per_capita_contracts" = "Contracts Per Capita",
                              "public_debt" = "Serving Public Debt",
                              "pub_transportation" = "Public Transportation",
                              "revenue_grants_share" = "Share of Revenue Grants",
                              "revenue_tax_share" = "Share of Revenue Tax", 
                              "rural_prop_tax" = "Rural Property Tax Rate",
                              "share_ccaa_culture" = "Culture (CCAA)", 
                              "share_ccaa_economic" = "Economic (CCAA)",
                              "share_ccaa_general_services" = "General Services (CCAA)",
                              "share_ccaa_housing_services" = "Housing Services (CCAA)",
                              "share_ccaa_security" = "Security (CCAA)", 
                              "share_ccaa_social_protection" = "Social Protection (CCAA)",
                              "share_contracts_agri" = "Share Contracts Agriculture Sector", 
                              "share_contracts_con" = "Share Contracts Construction Sector",
                              "share_contracts_indu" = "Share Contracts Industrial Sector", 
                              "share_contracts_serv" = "Share Contracts Service Sector",
                              "share_firms_service" = "Share Firms Service Sector", 
                              "share_ue_agriculture" = "Share Unemployment from Agriculture Sector",
                              "share_ue_construction" = "Share Unemployment from Construction Sector",
                              "share_ue_service" = "Share Unemployment from Service Sector",
                              "share_ue_industry" = "Share Unemployment from Industry Sector",
                              "social_services" = "Social Services", 
                              "ue_pct_2020" = "Unemployment Rate (2020)"))

Figure_5
```

```{r}
ggsave(plot = Figure_5,"Data_2020/Figures/Figure_5.png", dpi = 300, width = 10, height = 7, bg = "white")
```

## Data imputation

Using random forest, missing values are imputed. First all variables not used further are deleted.

```{r}
df_model <- 
  df_dea |> 
  select(-c(CODAUTO, dist_prov_cap, per_capita_CCAA, per_capita_MUN, per_capita_n_firms, median_inc_con_unit, party_mun, party_ccaa, diff_ue))
```

```{r}
m <- 5
set.seed(475)
imp <- mice(df_model, m = m, method= "rf")
df_model_imp <- complete(imp, action=m)
```

## Target variable evaluation

To start, off, the target variable is predicted using a simple random forest model, with different alterations of the target variable.

Every model is constructed the same, reducing features with a LASSO and training the model on a split data set, testing its performance on a test data set. All models have optimized m-try values, producing the best performing models while limiting computational time.

```{r}
df_model_robust <- df_model_imp |> 
  select(-c(INE, NOMBRE, CCAA, CPRO)) 
```

## Modelling - Target normal

### Lasso regression
```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train(dea_robust ~ .,
                     data = df_model_robust,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

#extracts coefficients
lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

# extracts names of variables with coefficients not zero
non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

# deleting the first name (constant)
non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "party")&!startsWith(non_zero_vars, "greater")]


df_model_final <- 
  df_model_imp |>
  dplyr::select(all_of(non_zero_vars), dea_robust, greater_20k, INE, NOMBRE) |> 
  drop_na()
# excluded , party_mun,party_ccaa for the moment

# all_of(non_zero_vars), dea_robust, greater_20k, party_mun, party_ccaa, INE, NOMBRE
```

#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_model_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data_pre_no  <- df_model_final[training.samples, ]
test.data_pre_no <- df_model_final[-training.samples, ] 

# deleting INE and NOMBRE from train set to keep in test only
train.data_pre_no$INE <- NULL
train.data_pre_no$NOMBRE <- NULL

```

#### Training RF

```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune <- train(dea_robust ~., 
                 data = train.data_pre_no,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(16)),
                 importance = TRUE)
```

##### Results

```{r}
test_results <- data.frame(dea_robust = test.data_pre_no$dea_robust)

#normal
test_results$rf <- predict(rf_tune, test.data_pre_no)
postResample(pred = test_results$rf,  obs = test_results$dea_robust)
```

## Modelling - Target log 

### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train(log(dea_robust) ~ .,
                     data = df_model_robust,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "party")&!startsWith(non_zero_vars, "greater")]


df_model_final <- 
  df_model_imp |>
  dplyr::select(all_of(non_zero_vars), dea_robust, INE, NOMBRE)|>
  drop_na()
# excluded , party_mun,party_ccaa for the moment
```

#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_model_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data_pre_log  <- df_model_final[training.samples, ]
test.data_pre_log <- df_model_final[-training.samples, ] 

# deleting INE and NOMBRE from train set to keep in test only
train.data_pre_log$INE <- NULL
train.data_pre_log$NOMBRE <- NULL

```

#### Training RF

```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune_log <- train(log(dea_robust) ~., 
                 data = train.data_pre_log,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(18)),
                 importance = TRUE)
```

##### Results

```{r}
# log
test_results$rf_log <- predict(rf_tune_log, test.data_pre_log)
test_results$rf_log <- exp(test_results$rf_log)
rf_log <- postResample(pred = test_results$rf_log,  obs = test_results$dea_robust)
```

## Modelling - Target log10

### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train(log10(dea_robust) ~ .,
                     data = df_model_robust,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "party")&!startsWith(non_zero_vars, "greater")]


df_model_final <- 
  df_model_imp |>
  dplyr::select(all_of(non_zero_vars), dea_robust, INE, NOMBRE)|> 
  drop_na()
# excluded party ccaa for the moment
```

#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_model_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data_pre_log10  <- df_model_final[training.samples, ]
test.data_pre_log10 <- df_model_final[-training.samples, ] 

# deleting INE and NOMBRE from train set to keep in test only
train.data_pre_log10$INE <- NULL
train.data_pre_log10$NOMBRE <- NULL

```

#### Training RF

```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune_log10 <- train(log10(dea_robust) ~., 
                 data = train.data_pre_log10,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(18)),
                 importance = TRUE)
```

##### Results

```{r}
# log10
test_results$rf_log10 <- predict(rf_tune_log10, test.data_pre_log10)
test_results$rf_log10 <- 10^(test_results$rf_log10)
rf_log10 <- postResample(pred = test_results$rf_log10,  obs = test_results$dea_robust)
```

## Modelling - Target cubed

### Lasso regression

```{r}
# Define the control parameters for LASSO regression
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train((dea_robust)^(1/3) ~ .,
                     data = df_model_robust,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "party")&!startsWith(non_zero_vars, "greater")]


df_model_final <- 
  df_model_imp |>
  dplyr::select(all_of(non_zero_vars), dea_robust, INE, NOMBRE)|> 
  drop_na()
# excluded party ccaa for the moment
```

#### Training and testing

```{r}
set.seed(123)  
training.samples <-
  df_model_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data_pre_cub  <- df_model_final[training.samples, ]
test.data_pre_cub <- df_model_final[-training.samples, ] 

# deleting INE and NOMBRE from train set to keep in test only
train.data_pre_cub$INE <- NULL
train.data_pre_cub$NOMBRE <- NULL
```

#### Training RF

```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune_cube <- train((dea_robust)^(1/3) ~., 
                 data = train.data_pre_cub,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(15)),
                 importance = TRUE)
```

##### Results

```{r}
# cube
test_results$rf_cube <- predict(rf_tune_cube, test.data_pre_cub)
test_results$rf_cube <- (test_results$rf_cube)^3
rf_cube <- postResample(pred = test_results$rf_cube,  obs = test_results$dea_robust)
```

### Modelling - Compare Results - in Text

Results are subsequently stored in one common data frame for a better overview.
```{r}
cube_df <- as.data.frame(t(rf_cube))
log_df <- as.data.frame(t(rf_log))
normal_df <- as.data.frame(t(rf_normal))
log10_df <- as.data.frame(t(rf_log10))

cube_df$model <- "Cube Root"
log_df$model <- "Log"
normal_df$model <- "Normal"
log10_df$model <- "Log10"

rf_metrics_table <- rbind(normal_df, log_df,  log10_df, cube_df)

rf_metrics_table
```

## MAE Source 

This chapter aims to find the sources of the Mean Absolute Error. To do that, absolute errors from the test set are joined with the original data-frame after adding the target variable.

```{r}
# normal
mae_per_observation_rf <- abs(test_results$rf - test_results$dea_robust)
test.data_pre_no$mae_rf <- mae_per_observation_rf 
```

```{r}
df_mae <- 
  test.data_pre_no |> 
  select(INE, NOMBRE, mae_rf) |> 
  left_join(df_dea, by = c("INE", "NOMBRE")) |> 
  drop_na()
```

### Efficiency and Absolute Error - In Text

First, a simple scatter plot provides an idea of how well extreme efficiency scores are predicted.

```{r}
Figure_6 <- 
  df_mae |> 
  ggplot()+
  geom_point(aes(x = dea_robust, y = mae_rf)) +
  theme_minimal() +
  labs(y = "Absolute Error", x = "Predicted Efficiency Score")

Figure_6
```

```{r}
ggsave(plot = Figure_6,"Data_2020/Figures/Figure_6.png", bg = "white", width = 10, height = 7, dpi = 300)
```

### Correlation Analysis - In Appendix

As before, a simple Spearman correlation matrix is passed into a `ggplot` object. This time the target variable is the absolute error of a given observation.

```{r}
df_corr_mae <-
  df_mae |> 
  select_if(is.numeric) |> 
  select(-mae_rf)

corr_mae<- cor(df_mae$mae_rf, df_corr_mae, method = "spearman")
corr_mae<-round(corr_mae,4)

df_corr_mae_results <- 
  data.frame(corr_mae) |> 
  pivot_longer(values_to = "correlation",
               cols = 1:69,
               names_to = "variable")


Figure_B6 <- 
  df_corr_mae_results |> 
  filter(correlation > 0.1 | correlation < -0.1) |> 
  ggplot(aes(x = variable, y = correlation)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "",
       y = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_x_discrete(labels = c("area_sqkm" = "Area in Square KM",
                              "average_age" = "Average Age", 
                              "business_turism_sme" = "Business, Tourism, and SMEs",
                              "education" = "Education",
                              "gov_bodies" = "Government Bodies",
                              "n_firms" = "Absolute Number of Firms",
                              "other_econ" = "Other Economic Spending",
                              "party_mun_id" = "Municipality Governing Party Ideology",
                              "pop_density" = "Population Density",
                              "pop_share_youth" = "Share of Youth",
                              "pop_total" = "Total Population",
                              "pub_safety_mobility" = "Public Safety and Mobility", 
                              "public_debt" = "Spending on Public Debt",
                              "share_ccaa_housing_services" = "Housing Services (CCAA)",
                              "share_ccaa_security" = "Security (CCAA)",  
                              "share_contracts_con" = "Share Contracts Construction Sector",
                              "share_essential_business" = "Share of Essential Businesses",
                              "share_firms_service" = "Share Firms Service Sector",
                              "share_women" = "Share Women", 
                              "social_services" = "Social Services",
                              "sports" = "Sports", 
                              "tax_fin_admin" = "Financial Administration"))

Figure_B6
```

```{r}
ggsave(plot = Figure_B6, "Data_2020/Figures/Figure_B6.png", bg = "white", dpi = 300, width = 10, height = 7)
```

### Mapping MAE - in Appendix 

As the efficiency score before, the absolute error for municipalities in the test data is mapped.

```{r}
esp_can <- esp_get_country() 
can_prov <- esp_get_can_provinces()
can_box <- esp_get_can_box() 
munic <- esp_get_munic() 

munic <-    
  munic |>  
  rename(INE = LAU_CODE)

df_mae_map <- munic |> 
  left_join(df_mae, by = "INE")  

Figure_B5 <- ggplot(esp_can) +
  geom_sf() +
  geom_sf(data = can_prov) +
  geom_sf(data = can_box) + 
  geom_sf(data = df_mae_map, aes(fill = mae_rf), color = "black", size = .5)+
  scale_fill_gradient(na.value = "grey", high ="#FF0000",low = "steelblue",name = "Absolute\nError")+
  theme_void()

Figure_B5
```

```{r}
ggsave(plot = Figure_B5, "Data_2020/Figures/Figure_B5.png", dpi = 300, bg = "white", width = 10, height = 7)
```

### Linear regression - Not included

A simple linear regression including all variables was run to find patterns. One distinct pattern was the effect of the information on provinces.

```{r}
df_mae_lm <-
  na.omit(df_mae) |> 
  select(-c(INE, NOMBRE, CCAA, party_ccaa, party_mun, median_inc_con_unit, target, dea_robust))
model <- lm(mae_rf ~ ., data = df_mae_lm)

# Print summary of the model
summary(model)
```

### Absoulte error by Province

#### In Text - Province

As such, the average absolute error by province is calculated, with apparent variations across provinces.

```{r}
CPRO_mae <- df_mae |> 
  group_by(CPRO) |> 
  summarise(mae_rf = mean(mae_rf, na.rm = TRUE))

# Create the graph
Figure_7 <- 
  ggplot(CPRO_mae, aes(x = CPRO, y = mae_rf)) +
  geom_bar(stat = "identity", fill = "steelblue", color = "black") +
  labs(x = "Regional Area Code",
       y = "Mean Absolute Error") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

Figure_7
```

```{r}
ggsave(plot = Figure_7,"Data_2020/Figures/Figure_7.png", bg = "white", width = 10, height = 7, dpi = 300)
```

A different but less striking visualization is its mapping.

```{r}
esp_can <- esp_get_country() 
can_prov <- esp_get_can_provinces()
can_box <- esp_get_can_box() 
provs <- esp_get_prov_siane(epsg = 3857)

df_mae_prov <- provs |> 
  left_join(CPRO_mae, by = c("cpro" = "CPRO"))  

ggplot(esp_can) +
  geom_sf() +
  geom_sf(data = can_prov) +
  geom_sf(data = can_box) + 
  geom_sf(data = df_mae_prov, aes(fill = mae_rf), color = "black", size = .5)+
  scale_fill_viridis_c(na.value = NA) +
  theme_minimal()
```

### In Appendix - Scatter plots

Scatter plots for municipality-size related variables with absolute errors were devised. Zooming in on relationships offers better insight and leads to the creation of three binary variables subsequently.

```{r}
pb1 <- 
  ggplot(df_mae) +
    geom_point(aes(x = pop_total, y = mae_rf)) +
    theme_minimal() +
    coord_cartesian(xlim = c(100, 2000), ylim = c(0, 0.18)) +
  labs(x = "Total Population",
       y = "Absolute Error")

```

```{r}
pb2 <- 
  ggplot(df_mae) +
    geom_point(aes(x = pop_density, y = mae_rf)) +
    theme_minimal() +
    coord_cartesian(xlim = c(0, 60), ylim = c(0, 0.18))+
  labs(x = "Population Density",
       y = "Absolute Error")
```

```{r}
pb3 <- 
  ggplot(df_mae) +
    geom_point(aes(x = area_sqkm, y = mae_rf)) +
    theme_minimal() +
    coord_cartesian(xlim = c(0, 300), ylim = c(0, 0.18))+
  labs(x = "Area in sqkm",
       y = "Absolute Error")
```

```{r}
Figure_B7 <- wrap_plots(pb1, pb2, pb3, ncol = 1)

Figure_B7
```

```{r}
ggsave(plot = Figure_B7, "Data_2020/Figures/Figure_B7.png", dpi = 300, bg = "white", height = 12, width = 10)
```

## Building the Model

### Feature Engineering

Province information is kept as a factor variable.

```{r}
df_ml <- 
  df_model_imp |> 
  select(-c(INE, NOMBRE, CCAA)) |> 
  mutate(CPRO = as.factor(CPRO))
```

According to the scatter plots above three binary variables are created using thresholds based on visual cues in said scatter plots.

`pop_density` --\> threshold: 50

`area_sqkm` --\> threshold: 50

`pop_total` --\> threshold: 500

Since municipality size seemingly plays such an important role, information on population size is interacted with all other numeric columns. Since we may expect population size to be highly correlated with with population density and area in sqkm, we would expect very similar interaction variables if interactions were applied with all three variables adding limited information. This was confirmed as, interacting all population size variables with all other numeric columns was tested but produced slightly worse results. Secondly, all municipality-size related variables are interacted with one another. Lastly, all variables are squared, cubed, and applied a logarithm.

To realize the first step, a function is created.
```{r}
generate_interactions <- function(df) {
  # only numeric columns except dea_robust (efficiency score)
  numeric_cols <- df |> 
    select(where(is.numeric), -dea_robust)
  
  # extracting municipality size columns - will be interacted manually
  mun_size_col <- numeric_cols |> 
    select(pop_total, pop_density, area_sqkm)
  
  # all other numeric columns excluding mun_size_col
  other_numeric_cols <- numeric_cols |> 
    select(-c(pop_total, pop_density, area_sqkm))
  
  # in a loop 
  interaction_cols <- list()
  
  for(col_name in names(other_numeric_cols)) {
    interaction_name <- paste("pop_total", col_name, sep = "_x_") # joint name is created
    interaction_name <- make.names(interaction_name, unique = TRUE)  # Ensure unique column names
    interaction_cols[[interaction_name]] <- mun_size_col[[1]] * other_numeric_cols[[col_name]] # only choosing pop_total
  }
  
  # Combining original and interaction columns
  df_interactions <- cbind(df, as.data.frame(interaction_cols))
  
  return(df_interactions)
}
```


```{r}
# to apply sqauring, cubing, and logarithm
vars_to_engineer <- 
  df_ml |> 
  select(where(is.numeric)) |> 
  names()

# feature engineering
df_ml <- 
  df_ml |> 
  mutate(pop_density_low = as.factor(if_else(pop_density < 50, 1,0)), # adding varaibles
         pop_total_low = as.factor(if_else(pop_total < 500, 1,0)),
         area_sqkm_low = as.factor(if_else(area_sqkm < 50, 1,0))) |>
  generate_interactions() |> 
  mutate(across(all_of(vars_to_engineer), ~log(. + 1), .names = "{.col}_log"), # feature engineering mathematical alteration of variables
         across(all_of(vars_to_engineer), ~.^2, .names = "{.col}_2"),
         across(all_of(vars_to_engineer), ~.^3, .names = "{.col}_3")) |> 
  mutate(pop_total_x_area_sqkm = area_sqkm*pop_total,
         pop_density_X_area_sqkm = pop_density * area_sqkm,
         pop_total_x_pop_density = pop_total*pop_density) |> 
  drop_na()

df_ml$dea_robust_2 <- NULL
df_ml$dea_robust_log <- NULL
df_ml$dea_robust_3 <- NULL
```

## Feature selection

### Lasso regression

For feature selection, a LASSO model is trained, as previously.
```{r}
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train the LASSO model
set.seed(123)
lasso_model <- train(dea_robust ~ .,
                     data = df_ml,
                     method = "glmnet",
                     trControl = ctrl,
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 100)))

lasso_coef <- coef(lasso_model$finalModel, s = lasso_model$finalModel$lambdaOpt)

non_zero_vars <- rownames(lasso_coef)[lasso_coef[,1]!=0]

non_zero_vars <- non_zero_vars[-1] 

non_zero_vars
```

Municipality-size related variables are kept because of importance found in the EDA.

```{r}
non_zero_vars <- non_zero_vars[!startsWith(non_zero_vars, "CPRO")&!startsWith(non_zero_vars, "greater")&!endsWith(non_zero_vars, "low1")]

df_ml_final <- 
  df_ml |> 
  dplyr::select(all_of(non_zero_vars), dea_robust, CPRO, area_sqkm_low, pop_density_low, greater_20k, pop_total_x_pop_density, pop_total_x_area_sqkm, pop_density_X_area_sqkm)
```

### Data splitting

Splitting the data set into training and testing data.

```{r}
set.seed(123)  
training.samples <-
  df_ml_final$dea_robust |> 
  createDataPartition(p = 0.75, list = FALSE)

train.data  <- df_ml_final[training.samples, ]
test.data <- df_ml_final[-training.samples, ] 
```

### Training Random Forest

```{r}
ctrl <- trainControl(method = "cv", number = 5)

set.seed(3456)
rf_tune_final <- train(dea_robust ~., 
                 data = train.data,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 1000, # found to be the best setting while computationally feasible
                 tuneGrid = data.frame(mtry= c(53)), # also found to be best tune
                 importance = TRUE)
```

Performance metrics are stored in a table.
```{r}
model_results <- data.frame(dea_robust = test.data$dea_robust)
model_results$rf <- predict(rf_tune_final, test.data)

# to store model performance metrics
performance_metrics_rf <- postResample(pred = model_results$rf, obs = model_results$dea_robust)
performance_metrics_rf
model_performance <- data.frame(performance_metrics_rf)
```

### Training Extreme Gradian Boosting

XGBoost is trained with parameters found to be working the best.

```{r}
ctrl <- trainControl(method = "cv", number = 5)

tune_grid <- expand.grid(
  nrounds = c(1500),
  max_depth = c(10),
  eta = c(0.005),
  gamma = c(0.000001),
  colsample_bytree = c(0.8),
  min_child_weight = c(5),
  subsample = c(0.7)
)

set.seed(123)
xgb_tune <- train(dea_robust ~., 
                  data = train.data,
                  method = "xgbTree",
                  preProc=c('scale','center'),
                  trControl = ctrl,
                  tuneGrid = tune_grid)
```

Results are appended to the model performance table.

```{r}
model_results$xg_boost <- predict(xgb_tune, test.data)
performance_metrics_xgb <- postResample(pred = model_results$xg_boost,  obs = model_results$dea_robust)
performance_metrics_xgb
model_performance <- cbind(data.frame(model_performance, performance_metrics_xgb))
```

### Training Catboost

```{r}
ctrl <- trainControl(method = "cv", number = 5)
```

```{r}
# Specify the features and the target
features <- names(df_ml_final)[names(df_ml_final) != "dea_robust"]
target <- "dea_robust"

# Convert data to catboost pool
train_pool <- catboost.load_pool(data = train.data[, features], label = train.data[[target]])
test_pool <- catboost.load_pool(data = test.data[, features],label = test.data[[target]])

fit_params <- list(iterations = 300,
                   learning_rate = 0.1,
                   depth = 10,
                   loss_function = 'RMSE',
                   l2_leaf_reg = 20,
                   use_best_model = TRUE,
                   od_type = "Iter") # stopping model training once overfitting

# Train the CatBoost model
catboost_tune <- catboost.train(train_pool, params = fit_params, test_pool = test_pool)
```

Again, metrics are appended to performance metrics data frame.

```{r}
# Make predictions on the test data
pred <- catboost.predict(catboost_tune, test_pool)
model_results$catboost <- pred

# storing it in model performance table
performance_metrics_cat <- postResample(pred = model_results$catboost, obs = model_results$dea_robust)
performance_metrics_cat
model_performance <- cbind(data.frame(model_performance, performance_metrics_cat))
```

### Model selection - In Text

Displayinf model performance metrics

```{r}
model_performance
```

## Interpretation

### Under and overachievers

Creating prediction intervals.

```{r}
# Predictions for train data using xgb_tune
predictions_test <- predict(xgb_tune, test.data)

# Create df_achievement_train with INE, NOMBRE, dea_robust
df_achievement_test <- cbind(df_model_imp[-training.samples, ], predictions_test) # Using training.samples from splitting data to correctly attach prediction data
df_achievement <- 
  df_achievement_test |> 
  rename(predictions = predictions_test)
```



```{r}
yhat <- df_achievement$predictions
y <- df_achievement$dea_robust
error <- y-yhat
noise <- error[1:100] # using error of first 100 observations as noise

# using remaining observations to calculate upper and lower bounds of interval for every observation
lwr <- yhat[101:length(yhat)] + quantile(noise,0.05, na.rm=T) 
upr <- yhat[101:length(yhat)] + quantile(noise,0.95, na.rm=T)

# all data into one data frame
predictions <- data.frame(real=y[101:length(y)], fit=yhat[101:length(yhat)], lwr=lwr, upr=upr)

predictions <- predictions |> 
  mutate(out=factor(if_else(real<lwr | real>upr,1,0)),# if real value is outside of interval = out
         o_u_achiever = factor(case_when(real < lwr ~ "underachiever",
                                         real > upr ~ "overachiever",
                                         TRUE ~ "as predicted")))


mean(predictions$out==1)
```

#### Prediction Interval - in Text

```{r}
Figure_8 <- 
  ggplot(predictions, aes(x=fit, y=real))+
  geom_point(aes(color=out)) + theme(legend.position="none") +
  geom_ribbon(data=predictions,aes(ymin=lwr,ymax=upr),alpha=0.3) +
  labs(x = "Predicted Efficiency",y="Recorded Efficiency") +
  scale_x_continuous(labels = scales::label_percent(), breaks = seq(0.60, 0.95, 0.05))+
  scale_y_continuous(labels = scales::label_percent(), breaks = seq(0.60, 1, 0.05))+
  theme_minimal() +
  theme(legend.position = "none") +
  scale_color_manual(values = c("#FF0000", "steelblue")) +
  annotate("text", x = 0.7, y = 0.95, label = "Overachiever", angle = 0, hjust = 1, color = "black", size = 3) + # overachiever
  geom_segment(aes(x = 0.7 , y = 0.948, xend = 0.748, yend = 0.925))+
  annotate("text", x = 0.85, y = 0.6, label = "Underachiever", angle = 0, hjust = 0, color = "black", size = 3) + # underachiever
  geom_segment(aes(x = 0.85 , y = 0.6, xend = 0.8009, yend = 0.655))+
  annotate("text", x = 0.65, y = 0.8, label = "As predicted", angle = 0, hjust = 1, color = "black", size = 3) + # as predicted
  geom_segment(aes(x = 0.65 , y = 0.8, xend = 0.675, yend = 0.737))

Figure_8
```

```{r}
ggsave(plot = Figure_8,"Data_2020/Figures/Figure_8.png", bg = "white", dpi = 300, width = 10, height = 7)
```

#### In Appendix - Names of Over and underachievers

Putting information on how municipalities fared into one data frame.

```{r}
df_achievement_subset <- df_achievement[101:nrow(df_achievement), ]

predictions_subset <- predictions[c("out", "o_u_achiever")]

df_achievement <- cbind(df_achievement_subset, predictions_subset)

unique(df_achievement$NOMBRE[df_achievement$o_u_achiever == "overachiever"])
```

```{r}
unique(df_achievement$NOMBRE[df_achievement$o_u_achiever == "underachiever"])
```

### Variable importance - in Text

Using variable importance data to plot in a `ggplot` object.

```{r}
var_imp <- varImp(xgb_tune)

# converting the variable importance data to a data frame
var_imp_df <- as.data.frame(var_imp$importance)
var_imp_df$Variables <- rownames(var_imp_df)

unique(var_imp_df$Variables)


# renamed labels of first thirty variables
new_labels <- c(
  "share_ccaa_security_3" = "Share of Security Spending in CCAA (^3)",
  "share_ccaa_general_services" = "Share of General Services Spending in CCAA",
  "share_ue_agriculture_log" = "Share of Unemployed from Agriculture Sector (Log)",
  "pop_share_foreigners" = "Share of Foreigners in Population",
  "ue_pct_2020_log" = "Unemployment Percentage 2020 (Log)",
  "pop_share_wk_age_log" = "Share of Working Age Population (Log)",
  "pop_share_youth_log" = "Share of Youth (Log)",
  "share_contracts_agri" = "Share of New Contracts in Agriculture Sector",
  "per_capita_contracts_log" = "Average New Contracts per Capita and Month (Log)",
  "pop_density_log" = "Population Density (Log)",
  "revenue_grants_share_3" = "Share of Revenue from Grants (^3)",
  "share_ccaa_economic_log" = "Share of Economic Spending in CCAA (Log)",
  "share_ccaa_general_services_2" = "Share of General Services Spending in CCAA (Log)",
  "pop_total_x_pop_density" = "Population Total x Density",
  "employment_promotion" = "Share of Employment Promotion Spending",
  "share_ue_industry" = "Share of Unemployment from Industry Sector",
  "share_ue_construction_log" = "Share of Unemployment from Construction Sector (Log)",
  "general_services_2" = "Share of General Services Spending (^2)",
  "share_firms_service_log" = "Share of Firms in Service Sector (Log)",
  "share_contracts_serv_3" = "Share of New Contracts per Capita and Month in Service Sector",
  "pop_density_X_area_sqkm" = "Population Density x Area (in SQKM)",
  "share_ue_wo_job_log" = "Share of Unemployed without a Prior Job (Log)",
  "housing_urban_plan_3" = "Share of Housing and Urban Planning Spending (^3)",
  "revenue_other_share_log" = "Share of Revenue from Different Sources (Log)",
  "area_sqkm_3" = "Area in SQKM (^3)",
  "social_services" = "Share of Social Services Spending",
  "pop_total_x_area_sqkm" = "Population Total x Area (in SQKM)",
  "gov_bodies_log" = "Share of Government Bodies Spending (Log)",
  "environment_log" = "Share of Environment Spending (Log)"
)

# Plot showing first thirty variables
Figure_9 <- 
  var_imp_df |>
  filter(Overall > 3.25) |> 
  ggplot(aes(x = reorder(Variables, Overall), y = Overall)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(x = "", y = "Variable Importance") +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 5.3))+ 
  scale_x_discrete(labels = new_labels)

Figure_9
```

```{r}
ggsave(plot = Figure_9,"Data_2020/Figures/Figure_9.png", bg = "white", width = 10, height = 7, dpi = 300)
```

### PDP

#### Policy-related Plots - in Text

Partial dependency data for the four most relevant policy-related variables is stored in a data-frame, plotting each plot into a `ggplot` object, while using `patchwork` to create one single figure containing all plots.

```{r}
# storing data on employment promotion
partial_plot_emp <- partial(xgb_tune, pred.var = "employment_promotion", plot = FALSE, rug = TRUE, train = train.data)
# Graph
p1 <- 
  ggplot(partial_plot_emp, aes(x = employment_promotion, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(x = "Budget Share for Employment Promotion",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank(),
        axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10)) + 
  scale_x_continuous(breaks = seq(0, max(partial_plot_emp$employment_promotion), by = 5), minor_breaks = NULL,  labels = scales::label_percent(scale = 1))+
  scale_y_continuous(labels = scales::label_percent())

# General services
partial_plot_gen <- partial(xgb_tune, pred.var = "general_services_2", plot = FALSE, rug = TRUE, train = train.data)
# Graph
p2 <- 
  ggplot(partial_plot_gen, aes(x = general_services_2, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(x = "Budget Share for General Services (²)",
       y = "Predicted Efficiency Score") +
  theme_minimal()+
  theme(axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10))+ 
  scale_y_continuous(labels = scales::label_percent())

# General services
partial_plot_soc <- partial(xgb_tune, pred.var = "social_services", plot = FALSE, rug = TRUE, train = train.data)
# Graph
p3 <- 
  ggplot(partial_plot_soc, aes(x = social_services, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(x = "Budget Share for Social Services",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank(),
        axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10)) + 
  scale_x_continuous(breaks = seq(0, max(partial_plot_soc$social_services), by = 10), minor_breaks = NULL, labels = scales::label_percent(scale = 1)) +
  scale_y_continuous(labels = scales::label_percent())

# Environmental spending
partial_plot_grants <- partial(xgb_tune, pred.var = "revenue_grants_share_3", plot = FALSE, rug = TRUE, train = train.data)
# Graph
p4 <- 
  ggplot(partial_plot_grants, aes(x = revenue_grants_share_3, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(x = "Share of Revenue from Grants (³)",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank(),
        axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10)) +
  scale_y_continuous(labels = scales::label_percent())
```

```{r}
Figure_10 <- p1 + p2 + p3 + p4

Figure_10
```

```{r}
ggsave(plot = Figure_10, "Data_2020/Figures/Figure_10.png", width = 10, height = 7, dpi = 300, bg = "white")
```

#### General Services and Pop Total - in Appendix

```{r}
Figure_B8 <- 
  ggplot(df_mae) +
    geom_point(aes(x = general_services, y = pop_total)) +
  labs(x = "Budget Share General Services", 
       y = "Total Population") +
  theme_minimal() +
  coord_cartesian( ylim = c(0, 50000)) +
  scale_x_continuous(breaks = seq(0, 100, by = 10), minor_breaks = NULL, labels = scales::label_percent(scale = 1))

Figure_B8
```

```{r}
ggsave(plot = Figure_B8, "Data_2020/Figures/Figure_B8.png", dpi = 300, bg = "white", width = 10, height = 7)
```

#### Structural-related predictors - in Text

The same is done for structural-related predictors.

```{r}
# Security CCAA
partial_plot_sec_ccaa <- partial(xgb_tune, pred.var = "share_ccaa_security_3", plot = FALSE, rug = TRUE, train = train.data)
# Graph
p5 <- 
  ggplot(partial_plot_sec_ccaa, aes(x = share_ccaa_security_3, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(x = "Budget Share CCAA Security (³)",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank(),
        axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10))+
  scale_y_continuous(labels = scales::label_percent())

# CCAA General services
partial_plot_gen_ccaa <- partial(xgb_tune, pred.var = "share_ccaa_general_services", plot = FALSE, rug = TRUE, train = train.data)
# Graph
p6 <- 
  ggplot(partial_plot_gen_ccaa, aes(x = share_ccaa_general_services, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(x = "Budget Share CCAA General Services",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank(),
        axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10))+
  scale_y_continuous(labels = scales::label_percent())

# Share of foreigners
partial_plot_foreigners <- partial(xgb_tune, pred.var = "pop_share_foreigners", plot = FALSE, rug = TRUE, train = train.data)
# Graph
p7 <- 
  ggplot(partial_plot_foreigners, aes(x = pop_share_foreigners, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(x = "Share of Foreigners",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank(),
        axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10))+
  scale_y_continuous(labels = scales::label_percent()) + 
  scale_x_continuous(breaks = seq(0, max(partial_plot_foreigners$pop_share_foreigners), by = 10), minor_breaks = NULL,  labels = scales::label_percent(scale = 1))

# Unemployment rate (2020)
partial_plot_ue <- partial(xgb_tune, pred.var = "ue_pct_2020_log", plot = FALSE, rug = TRUE, train = train.data)
# Graph
p8 <- 
  ggplot(partial_plot_ue, aes(x = ue_pct_2020_log, y = yhat)) +
  geom_line(color = "steelblue") +
  labs(x = "Unemployment Percentage 2020 (log)",
       y = "Predicted Efficiency Score") +
  theme_minimal() +
  theme(axis.ticks = element_blank(),
        axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10)) +
  scale_y_continuous(labels = scales::label_percent())
```

```{r}
Figure_11 <- p5 + p6 + p7 + p8

Figure_11
```

```{r}
ggsave(plot = Figure_11,"Data_2020/Figures/Figure_11.png", width = 10, height = 7, dpi = 300, bg = "white")
```

### SHAP

#### Importance Bee Plot - Not Included

SHAP values are estimated using `shapviz`.

Bee plot is only used for a glance at how SHAP values behave.

```{r}
X <- train.data[sample(nrow(train.data), 1000), -which(names(train.data) == "dea_robust")]
bg_X <- train.data[sample(nrow(train.data), 100), -which(names(train.data) == "dea_robust")]

s <- kernelshap(xgb_tune, X = X, bg_X = bg_X) 
sv <- shapviz(s)
sv_importance(sv, kind = "bee", max_display = Inf) +
  theme_minimal()
```

#### Dependence Plot - Structural Features - in Text

Dependency plots are altered through looping through the chosen predictors - in this case structural predictors.

```{r}
SHAP_structural <- c("pop_share_foreigners", "ue_pct_2020_log", "pop_density_log")

# creating list from which information is taken during the loop
customizations_structural <- list(
  list(
    legend_title = "Share of Unemployed\nfrom Agricultural Sector (log)",
    x_axis_label = "Share of Foreigners"

  ),
  list(
    legend_title = "Share of\nPopulationin Working Age (log)",
    x_axis_label = "Unemployment Rate (log)"

  ),
  list(
    legend_title = "Budget Share CCAA Security (³)",
    x_axis_label = "Population Density (log)"

  )
)

# for every variable stored in the object SHAP_structural the function is applied
plots_structural <- lapply(seq_along(SHAP_structural), function(i) {
  var <- SHAP_structural[i]
  customizations_structural <- customizations_structural[[i]] # takes i-spot in the list
  
  plot <- sv_dependence(sv, v = var) + # applies cosmetic alterations to the plots
    theme_minimal() +
    labs(colour = customizations_structural$legend_title,
         x = customizations_structural$x_axis_label) +
    guides(colour = guide_colourbar(direction = "horizontal", title.position = "top"))
  
  return(plot)
})

# Display the first plot as an example
p9 <- 
  plots_structural[[1]] + #plots are stored in a list - hence must be chosen from that list
  scale_x_continuous(breaks = seq(0, 65, by = 10), minor_breaks = NULL,  labels = scales::label_percent(scale = 1))


p10 <- 
  plots_structural[[2]]

p11 <- 
  plots_structural[[3]]


Figure_12 <- wrap_plots(p9, p10, p11, ncol = 1)

Figure_12
```

```{r}
ggsave(plot = Figure_12,"Data_2020/Figures/Figure_12.png", width = 10, height = 12, dpi = 300, bg = "white")
```

#### Dependence plots - Provinces - in Text

Since only one variable is required in this step, all is done manually.

```{r}
SHAP_region <- "CPRO"
Figure_13 <- 
  sv_dependence(sv, v = SHAP_region) +
  theme_minimal()+
  guides(colour = guide_colourbar(direction = "horizontal", title.position = "top")) +
  labs(color = "Share of Youth (log)",
       x = "Province Codes")

Figure_13
```

```{r}
ggsave(plot = Figure_13,"Data_2020/Figures/Figure_13.png", dpi = 300, width = 12, height = 7, bg = "white")
```

#### Dependence Plots - Policy Features - in Text

As before, variables of interest are stored in an object, customization are stored in a list, and plots are created by looping through a function.

```{r}
SHAP_policy <- c("employment_promotion", "environment_log","social_services","general_services_2")

customizations_policy <- list(
  list(
    legend_title = "Share of Unemployed\nfrom Agricultural Sector (log)",
    x_axis_label = "Budget Share Employment Promotion"

  ),
  list(
    legend_title = "Share of Contracts\nin Agricultural Sector",
    x_axis_label = "Budget Share Environment (log)"

  ),
  list(
    legend_title = "Share of Contracts\nin Agricultural Sector",
    x_axis_label = "Budget Share Social Services"

  ),
  list(
    legend_title = "Share of Contracts\nin Construction Sector (log)",
    x_axis_label = "Budget Share General Services (²)"

  )
)

plots_policy <- lapply(seq_along(SHAP_policy), function(i) {
  var <- SHAP_policy[i]
  customizations_policy <- customizations_policy[[i]]
  
  plot <- sv_dependence(sv, v = var) + 
    theme_minimal() +
    labs(colour = customizations_policy$legend_title,
         x = customizations_policy$x_axis_label) +
    guides(colour = guide_colourbar(direction = "horizontal", title.position = "top"))
  
  return(plot)
})


p12 <- 
  plots_policy[[1]] + 
  scale_x_continuous(breaks = seq(0, 50, by = 10), minor_breaks = NULL,  labels = scales::label_percent(scale = 1))

p13 <- 
  plots_policy[[2]]

p14 <- 
  plots_policy[[3]] + 
  scale_x_continuous(breaks = seq(0, 50, by = 10), minor_breaks = NULL,  labels = scales::label_percent(scale = 1))

p15 <- 
  plots_policy[[4]] 

Figure_14 <- p12 + p13 + p14 + p15

Figure_14
```

```{r}
ggsave(plot = Figure_14,"Data_2020/Figures/Figure_14.png", width = 14, height = 10, dpi = 300, bg = "white")
```

## Appendix

### Historical Unemployment Rate Development

Unfortunately, the data recovered from the World Bank Group is very messy. Hency data is read into the environment line by line.

```{r}
# reading file line by line
lines <- readLines("Data_2020/world_ue_data.csv", encoding = "UTF-8")

# column names from the third line
column_names <- unlist(strsplit(lines[5], ",", fixed = TRUE))
column_names <- gsub('"', "", column_names)  # Removing quotes
column_names <- gsub("\\\\", "", column_names)

# shifting columns one column to the right
column_names <- c("country_name", column_names)


# using commas to split lines into columns
data <- lapply(lines[-(1:5)], function(line) {
  unlist(strsplit(line, ",", fixed = TRUE))
})

# converting list into data frame and assigning column names
data <- as.data.frame(do.call(rbind, data))
names(data) <- column_names


Figure_1 <- 
  data |> 
  select("country_name","1990":"2021") |> 
  mutate(across("1990":"2021", ~str_extract(., "\\d+\\.?\\d*"))) |> 
  filter(country_name == '"Spain') |> 
  pivot_longer(cols = "1990":"2021",
               values_to = "ue_rate",
               names_to = "year") |> 
  mutate(ue_rate = as.numeric(ue_rate)/100,
         year = as.Date(paste0(year, "-01-01"))) |> 
  ggplot()+
  geom_line(aes(x = year, y = ue_rate), colour = "steelblue") +
  geom_vline(xintercept = as.Date("2007-02-01"), linetype = "dashed", color = "black") + # begin o great recession
  annotate("text", x = as.Date("2004-07-01"), y = 0.23, label = "Begin of the\nGreat Recession", angle = 0, hjust = 1, color = "black", size = 3) + 
  geom_segment(aes(x = as.Date("2007-02-01"), y = 0.23, xend = as.Date("2004-08-01"), yend = 0.23)) +
  geom_vline(xintercept = as.Date("2012-12-01"), linetype = "dashed", color = "black") + # End of Eurozone crisis
  annotate("text", x = as.Date("2014-12-20"), y = 0.1, label = "End of the\nEurozone Crisis", angle = 0, hjust = 0, color = "black", size = 3) + 
  geom_segment(aes(x = as.Date("2012-12-01"), y = 0.1, xend = as.Date("2014-12-01"), yend = 0.1)) +
  geom_vline(xintercept = as.Date("2019-05-01"), linetype = "dashed", color = "black") + # Start of covid
  annotate("text", x = as.Date("2018-01-01"), y = 0.25, label = "Begin of Covid-19\nPandemic", angle = 0, hjust = 1, color = "black", size = 3) + 
  geom_segment(aes(x = as.Date("2019-05-01"), y = 0.23, xend = as.Date("2018-01-01"), yend = 0.25)) + 
  labs(y = "Share (%)",
       x = "") +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_x_date(date_labels = "%Y", breaks = "1 year", expand = c(0, 0))+ 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + # creating shaded areas
  annotate(
    geom = "rect",
    xmin = as.Date("2007-02-10"), xmax = as.Date("2012-12-01"),
    ymin = -Inf, ymax = Inf,
    alpha = 0.5, fill = "grey"
  )+
  annotate(
    geom = "rect",
    xmin = as.Date("2019-05-10"), xmax = as.Date("2021-12-31"),
    ymin = -Inf, ymax = Inf,
    alpha = 0.5, fill = "grey"
  )

Figure_1
```

```{r}
ggsave(plot = Figure_1,"Data_2020/Figures/Figure_1.png", dpi = 300, bg = "white", width = 10, height = 7)
```

### Visualizing missing municipalities

Where is the difference

```{r}
# for every community in data frame used in this project count absolute number of municipalities
m <-
  df_dea |>
  mutate(CODAUTO_data = CODAUTO) |> 
  group_by(CODAUTO_data, CCAA) |> 
  distinct(INE) |> 
  count(CODAUTO_data)

# adding missing communities to join data
new_row <- data.frame(CODAUTO_data = c("18","19"), CCAA = c("Melilla", "Ceuta"),n = c(0, 0))
m <- m |> 
  bind_rows(new_row)

# reading municipality declaration data
codigo_municipio <- read_excel("Data_2020/diccionario24.xlsx", skip = 1, col_names = TRUE) |> 
  mutate(INE = paste0(CPRO, CMUN),
         Provincia = as.integer(CPRO))

# for every community in the official registry counting total number of municipalities
o <- 
  codigo_municipio |>
  mutate(CODAUTO_original = CODAUTO) |> 
  group_by(CODAUTO_original) |> 
  distinct(INE) |> 
  count(CODAUTO_original)

# joining data and estimating the difference
difference_n_mun <- o |> 
  left_join(m, by = c("CODAUTO_original"= "CODAUTO_data")) |> 
  mutate(difference = n.x - n.y,
         proportion = (difference/n.x)*100)

# plotting the difference by autonomous community
Figure_B1 <- 
  difference_n_mun |> 
  filter(!CCAA == "Ceuta" & !CCAA == "Melilla") |> 
  ggplot()+
  geom_col(aes(CCAA, proportion), fill = "steelblue") +
  geom_text(aes(CCAA, proportion, label = paste(round(proportion, 2),"%")),
            vjust = -0.4,
            size = 2.5) + 
  scale_y_continuous(labels = (scales::label_percent(scale = 1))) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(y = "Share (%)",
       x = "")

Figure_B1
```

```{r}
ggsave(plot = Figure_B1, "Data_2020/Figures/Figure_B1.png", dpi = 300, width = 10, height = 7, bg = "white")
```

### Data summary

```{r}
# creating summary
summary_df <- psych::describe(df_summary)

# adding information on variable names
summary_df$variable <- rownames(summary_df)
# variable column to beginning
summary_df <- summary_df[, c("variable", colnames(summary_df)[1:(ncol(summary_df)-1)])]

#removing columns
summary_df <- summary_df[, !colnames(summary_df) %in% c("trimmed", "mad", "skew", "kurtosis")]

# adding information into flextable object
ft <- flextable(summary_df)

ft <- theme_vanilla(ft) |> 
  autofit() |> 
  set_table_properties(width = 0.8, layout = "autofit") |> # smaller table
  align(j = 2:ncol(summary_df), align = "center", part = "all") |>   #  text in all columns centered except the first
  colformat_double(j = 2:ncol(summary_df), digits = 2) |>  # 2 decimal places
  fontsize(size = 8, part = "all") 

# creating word doc
doc <- read_docx()

# adding flextable to word doc
doc <- body_add_flextable(doc, value = ft)

# saving word document
print(doc, target = "Data_2020/Tables/summary_table.docx")
```

### Template

For the tables in the final table, a template table is created.

```{r}
# creating an empty data frame
empty_df <- summary_df[1:6, ]
empty_df[] <- NA  # filling with NA

# flextable object from empty table
ft_empty <- flextable(empty_df)

# customizing to align with summary statistics table 
ft_empty <- ft_empty |> 
  theme_vanilla() |> 
  autofit() |> 
  set_table_properties(width = 0.8, layout = "autofit") %>%
  align(j = 2:ncol(empty_df), align = "center", part = "all")

doc_empty <- read_docx()

doc_empty <- body_add_flextable(doc_empty, value = ft_empty)

print(doc_empty, target = "Data_2020/Tables/empty_template_table.docx")
```
